%%% ArXiv template from https://www.overleaf.com/latex/templates/an-arxiv-template/gbzmznbxvwpr

\documentclass[10pt]{article}
\usepackage{graphicx}
\baselineskip=16pt

\usepackage{indentfirst,csquotes}

\topmargin= .5cm
\textheight= 20cm
\textwidth= 32cc
\baselineskip=16pt

\evensidemargin= .9cm
\oddsidemargin= .9cm

\usepackage{amssymb,amsthm,amsmath}
\usepackage{xcolor,paralist,hyperref,fancyhdr,etoolbox}


\newtheorem{theorem}{Theorem}[]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}


\hypersetup{colorlinks=true, linkcolor=black, filecolor=black, urlcolor=black }
\def\proof{\noindent {\it Proof. $\, $}}
\def\endproof{\hfill $\Box$ \vskip 5 pt }









%%% PERSONAL ADD-ONS
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\allowdisplaybreaks
\numberwithin{equation}{section}
%\usepackage{natbib}
\include{macros}

% For foot notes in author name
\newcommand{\footremember}[2]{%
	\footnote{#2}
	\newcounter{#1}
	\setcounter{#1}{\value{footnote}}%
}
\newcommand{\footrecall}[1]{%
	\footnotemark[\value{#1}]%
} 

\begin{document}
	
	
	\title{Ongoing work on MCWAL} %%%%%%%%%%%%
	\author{Pierre Borie\footremember{1}{University of Montreal, Department of Computer Science and Operations Research, Montreal, QC, Canada}}
	\date{}
	
	
	
	\maketitle
	\tableofcontents
	
	\begin{abstract}
		\noindent This informal document deals with the ongoing work and thinking on a algorithm for constrained nonlinear least squares. The current algorithm  name is MCWAL for Moindres Carr\'es With Augmented Lagrangian. Some sections about more general aspects about nonlinear programming might also appear here and there. The aim of this document is to reflect the thinking and understanding of different kinds of notions. 
	\end{abstract} %%%%%%%%% 
	

	\section{Introduction}\label{sec:intro}
	
	We consider least squares problems subject to both nonlinear and linear constraints of the form
	\begin{equation}
		\label{eq:model_cnls}
		\begin{aligned}
			\min_{x\in \RR^n} \quad & \dfrac{1}{2} \|r(x)\|^2 \\
			\text{s.t.} \quad & h(x) = 0 \\
			& \scal{c_i}{x} = b_i,\quad i=1,\ldots,m \\
			& l \le x \le u,
		\end{aligned}
	\end{equation}
	where $r\colon \RR^n \to \RR^d$  and $h\colon \RR^n \to \RR^t$ are assumed to be nonlinear, potentially non convex, continuously differentiable functions, $\scal{\cdot}{\cdot}$ is the canonical inner product and $\|\cdot\| $  its induced euclidean norm, $c_i$ are $m$  independent vectors of $\Real^n$, $( m \le n)$, $b=(b_1,\ldots,b_m)^T \in \RR^m$ and $\ell$ and $u$ are vectors in $\RR^n$. Without loss of generality, some components of the latter two vectors can be set to $\pm \infty$ for unbounded parameters. In the context of least squares problems, components $r_i$ of the function $r$ are often denoted as the residuals.
	
	We will also refer to the linear constraints using the following set notation 
	
	\begin{equation}
		\label{eq:linear_constraints}
		\calX = \left\{ x \in \RR^n \ | \ Cx=b,\ l \le x \le u\right\},
	\end{equation}
	where $C$ is the matrix whose columns are the vectors $c_i$. By linear independence of those vectors, $C$ is a full rank matrix. The set $\calX$ is thus convex.
	
	\section*{Notations}
	 
	
	The Jacobian matrix of constraints function $h$ is noted $A$.
	
	When considering iterative methods for solving problem~\eqref{eq:model_cnls}, $k$ will, if not mentioned otherwise, refer to the iteration number. In order to simplify notations, quantities relative to a given iteration will be noted with the iteration number as an index, such as $x_k$ for the iterate, $r_k$ for $r(x_k)$ ,$J_k$ for $J(x_k)$ etc.
	
	Symbol $:=$ shall be used to state the definition of a numerical object (function, vector etc.)
	
	For a set $E$, a sequence $(x_k)_k$ converging to $x^* \in E$ when $k\to+\infty$, we will write $x_k \to_E x^*$ if, above a certain index, all $x_k$ are in $E$. 
	
	The set of linear combinations of a set of vectors $(v_1,\ldots,v_n)$ is written $\rangle$ $span(v_1,\ldots,v_n)$. 
	
	$\calM_{m,n}(\bbK)$ denotes the set of matrices with $m$ rows and $n$ columns whose coefficients are in the field $\bbK = \RR$ or $\CC$. In practice, it will always be $\RR$ and would be surprising to consider the complex case but one never knows. When $m=n$, i.e. for square matrices, we will write $\calM_n(\bbK)$. The adjoint of a matrix $A\in \calM_{m,n}(\bbK)$ is written $A^*$. In the real case, it corresponds to its transpose, in the complex case, it corresponds to the tranpose if its conjugate. 
	
	We also introduce notations for special subsets of matrices. $GL_n(\bbK)$, also known as the linear group of order $n$, denotes the set of invertible matrices (or "non-singular"...). $O(n)$ is the orthogonal group, i.e. the set of matrices $Q$ in $\calM_n(\RR)$ such that $Q^TQ=I_n$. $\calS_n(\bbK)$ denotes the set of matrices equal to their adjoint. In the real case, such matrices are called symmetric, in the complex case, they are called hermitian.  
	
	If $H$ is a symmetric matrix, i.e. $H^T=H$, we will write $H \succ 0$, resp. $H \succeq 0$, when $H$ is positive definite, resp. semi-definite positive. In the case $H\succ 0$, the bilinear mapping $\scal{\cdot}{\cdot}_H:(x,y) \mapsto \scal{x}{Hy} $ forms an inner product of induced norm $\|\cdot\|_H:x\mapsto \sqrt{\scal{x}{Hx}}$.
	
	
	\section{Optimality conditions for nonlinear programming}
	
	In this section, we describe optimality conditions for nonlinear programs more general than~\eqref{eq:model_cnls} from different views.
	
	\subsection{An algebraic view}
	
	We consider the general mathematical program\footnote{Shall I write the KKT conditions w.r.t this formulation and then consider multipliers for the bounds?}
	\begin{equation}
		\label{eq:math_prog}
		\begin{aligned}
			\min_x \quad & f(x) \\
			\text{s.t.} \quad & h(x)=0 \\
			& l \le x \le u,
		\end{aligned}
	\end{equation}
	with the same assumptions on of differentiability of functions $f$ and \(h\). We introduce the Lagrangian associated to problem~\eqref{eq:math_prog}:
	\begin{equation}
		\label{eq:lagrangian}
		\ell(x,\lambda) = f(x) + \scal{\lambda}{h(x)},
	\end{equation}
	where $\lambda$ is the vector of Lagrange multipliers. Algorithms discussed aim to find local minimum of problem~\eqref{eq:math_prog}, i.e. feasible and of minimal value in s neighborhood. Under suitable assumptions, one can establish necessary and even sufficient conditions for local optimality. One of the most important assumptions belongs to the familty of constraints qualifications. The following is the one we will employ.
	
	\begin{definition}\label{def:licq}
		\textbf{(LICQ)}
		The LICQ holds at $x^*$ if the gradients of the equality constraints evaluated at $x^*$ are linearly independent. In other terms, the matrix $A(x)$ is full rank.
	\end{definition}
	
	Using this and standard differentiability assumptions, one can state first-order necessary optimality conditions, also known as KKT conditions. 
	
	\begin{definition}\label{def:kkt_point}
		\textbf{(KKT conditions)}
		A point $x^*$ satisfies the KKT conditions if $x^* $ is feasible and there exists multipliers \(\lambda^*\) such that \[\nabla_x \ell (x^*,\lambda^*)=0.\]
	\end{definition}
	
	
	A point satisfying those conditions if also said to be a KKT point or a first order critical point for problem~\eqref{eq:math_prog}. Depending on the context, we would refer to a KKT point either by just wrting $x^*$ or the couple formed after $x^*$ and its associated Lagrange multiplier $\lambda^*$The necessary conditions follow.
	
	\begin{theorem}\label{theo:fonc}
		\textbf{(First Order Necessary Conditions)\cite[][Theorem 12.1]{nocedalwright:2006}}
		Let $x^*$ be a local solution of~\eqref{eq:math_prog} at which the LICQ holds. Then $x^*$ is a KKT point.
	\end{theorem}
	
	Sufficient optimality conditions can be established using second order information.
	
	\begin{definition}\label{def:soc}
		\textbf{(Second Order Conditions)}
		A point $(x^*,\lambda^*)$ satisfies the second order conditions if it is a KKT point for ~\eqref{eq:math_prog} at which the LICQ holds and if the matrix $\nabla_{xx}^2\ell(x^*,\lambda^*)$ is positive definite on the null space of the constraints Jacobian, i.e.:
		\[\forall w \text{ verifying } \scal{A(x^*)}{w}=0,\ \scal{w}{\nabla_{xx}^2\ell(x^*,\lambda^*)w} > 0.\]
	\end{definition}
	
	\begin{theorem} \textbf{(Second Order Sufficient Conditions) \cite[][Theorem 12.5]{nocedalwright:2006}}
		If $x^*$ satisfies the second order conditions, then $x^*$ is a local minimum of~\eqref{eq:math_prog}.
	\end{theorem}
	
	\subsection{A geometric view}
	
	We now consider the general program
	
	\begin{equation}
		\label{eq:math_prog_geom}
		\begin{aligned}
			\min_x \quad & f(x) \\
			\text{s.t.} \quad & x \in \calC,
		\end{aligned}
	\end{equation}
	
	where the feasible set $\calC$ is a (of course non empty), subset of $\RR^n$. The idea is to describe optimality conditions using inclusions of certain vectors to sets and cones. General results can be stated without any further assumptions on $\calC$. Yet, we will give important results about projections that require to assume $\calC$ is convex. As we shall see in section~\ref{sec:about_al}, our method will imply solving quadratic programs subject to linear, hence convex, constraints. Most of the content exposed is from~\cite[][Chapter 12]{nocedalwright:2006}.
	
	We start by defining two important sets, that are actually cones. We remind that a set $K$ is a cone if for all $x \in K$ and $\alpha >0 $, $\alpha x\in K $. A cone is pointed if it contains $0$.
	
	\begin{definition}\textbf{Tangent Vector}
		Let $x\in \calC$. A vector $v\in \RR^n$ is said to be a tangent vector to $\calC$  at $x$ if there are sequences $(x_k)_k \to_\calC x$ and $(t_k)_k \searrow 0^+$ such that 
		\[\lim\limits_{k\to \infty} \dfrac{x_k-x}{t_k}=v.\]
	\end{definition}
	
	\begin{definition}
		
	The \textbf{tangent cone} is the set of all tangents vectors to $\calC$ at $x$ and is denoted by $T_\calC(x)$.
	\end{definition}
	
	For a better understanding, the limit in the definition can be written as the relation
	\[x_k - x = t_kv + o(1).\]
	Intuitively, tangent vectors are all the directions going from $x$ that stay in $\calC$, up to a scaling by a very small positive factor. In the context of constrained optimization, they characterize directions in which one can step away from $x$ while remaining feasible. 
	
	\textbf{Remark} 
	
	Rigorously, it is a $o(t_k)$ but since $t_k \to 0$, a $o(t_k)$ is also a $o(1)$. Our intent is to highlight the  visualization of a tangent vector as a scaled $x_k-x$ for $(x_k)_k \to x$.
	
	\begin{definition}
		
		The \textbf{normal cone} to $\calC$ at $x$ is the set defined by
		\[N_\calC (x) := \left\{w \in \RR^n\ | \ \forall v \in T_\calC(x), \scal{v}{w} \le 0\right\}.\]
	\end{definition}
	
	The normal cone is merely the orthogonal complement of the tangent cone and is involved in the following first order optimality condition.
	
	\begin{theorem}\label{theo:first_order_normal_cone}
		If $x^*$ is a local minimum of $f$, then \[-\nabla f(x^*) \in N_\calC(x^*).\]
	\end{theorem}
	
	This theorem reflects the intuitive fact that a local minimum is a point from which the objective function cannot be reduced while remaining feasible. While this result is simple is elegant, its use is limited by the difficulty to express both tangent and normal cones without any other assumptions. For instance, with the LICQ, the tangent cone can be expressed as an intersection of hyperplanes depending on the constraints gradients and combining theorem~\ref{theo:first_order_normal_cone} with a Farkas-like lemma gives the KKT conditions. Similar consequences occur when the feasible set is convex, which we will assume from now on.
	
	We first remind the definition a convex set.
	
	\begin{definition}
		A set $\calC\subseteq \RR^n$ is \textbf{convex} if \(\forall (x,y) \in \calC^2\) and \(\ \alpha \in [0,1] \), one has \(\alpha x+ (1-\alpha)y \in \calC\).
		In other words, the line segment between two points remains in $\calC$, for all points of $\calC$.
	\end{definition}
	
	In this setting, the normal cone can be described more simply.
	
	\begin{proposition}\label{prop:normal_set_convex}
		Let $\calC $ a non empty convex set. Then for all $x \in \calC$:
		\[N_\calC (x) = \left\{v \ | \ \forall y \in \calC, \scal{v}{x-y} \ge 0\right\}.\]
		For $x \notin \calC$, $N_\calC (x) = \emptyset$.
	\end{proposition}
	
	Then, if $x^*$ is a local minima of~\eqref{eq:math_prog_geom}, the inclusion in theorem~\ref{theo:first_order_normal_cone}  becomes
	\begin{equation}\label{eq:fo_optimality_convex_set}
		\forall x \in \calC, \scal{\nabla f(x^*)}{x-x^*} \ge 0.
	\end{equation}
	
	\textbf{Remarks on~\eqref{eq:fo_optimality_convex_set}}
	\begin{enumerate}
		\item This condition if sufficient when the function $f$ is convex
		\item It also reflects the very visual fact that in $\RR^2$, the line tangent to a local minima is below the curve
	\end{enumerate}
	
	So we now have a more simple way to characterize a minimum but it is still not suited to be used in algorithms. Obviously, one cannot check the sign of an infinite number of inner product but more fundamentally, algorithm must be able to manipulate points that are likely to be outside of the feasible set. Using projections is the key.
	
	\begin{definition}\label{def:projection}
		
		Let $\calC \subseteq \RR^n$. The \textbf{projection operator} is the set-value mapping $P_\calC \colon \RR^n \rightrightarrows \calC $ defined by
		\[ P_C(\bar{x}) = \argmin_{y \in \calC}  \dfrac{1}{2} \|\bar{x}-y\|^2.\]
	\end{definition}
	
	For this definition, we introduce the notion of set-value mapping~\cite{rockafellarwets:1998} to remain general but in the convex case, it is single valued and well defined if the concerned set is closed. 
	
	\begin{theorem}
		Let $\calC$ be a close convex set and $x \in \RR^n$. Then
		\[\bar{x} = P_\calC(x) \iff \forall y \in \calC, \scal{\bar{x}-x}{y-\bar{x} \ge 0.}\]
	\end{theorem}
	
	Since the norm is convex, this theorem is simply derived from the first order necessary optimality  conditions. Also the inequality kind of looks like the inequality in proposition~\eqref{prop:normal_set_convex}, which motivates the following theorem.
	
	\begin{theorem}
		Let $\calC$ be a close convex set, $x \in \RR^n$ and $z \in \calC$. Then
		\[z = P_\calC(x) \iff (x-z) \in N_\calC(z).\]
	\end{theorem}
	
	From this result, one deduces the following useful corollary.
	
	\begin{corollary}
		Let $\calC$ be a close convex set, $x \in \calC$ and $z \in \RR^n$. If $z \in N_\calC(x)$, then $\forall t\ge 0$, $P_\calC(x+tz)=x$.
	\end{corollary}
	
	Applying the above corollary to opposite of the gradient of a local minimum, thus verifying theorem~\ref{theo:first_order_normal_cone}, gives the next theorem.
	
	\begin{theorem}
	Consider the problem~\eqref{eq:math_prog_geom} where the non empty feasible set $\calC$ is closed and convex. Then $x^*$ is a local minimum if and only if for all $t \ge 0$, $P_\calC\left(x^*-t\nabla f(x^*)\right)=x^*$.
	\end{theorem}
	
	There is our desired result that we shall use as a termination criteria, up to a small tolerance. In practice, we will work with the Cauchy arc defined as
	
	\begin{equation}\label{eq:cauchy_arc}
		p(t,x) := P_\calC(x-t\nabla f(x)),
	\end{equation}
	
	that is the path alongside the steepest descent direction. The Cauchy point is the point minimizing the objective function on the Cauchy arc, i.e.:
	
	\begin{equation}
		x^C := x - t^C \nabla f(x) \text{ where } t^C = \argmin_{t \ge 0} f\left(p(t,x)\right).
	\end{equation}
	
	If an algorithm uses the Cauchy point as the new iterate, then the algorithm would converge. Moreover, if each iterate reduces the objective function as much as a fraction of the Cauchy point, the algorithm would also converge. This pretty much sets the foundation, and intuition, for the theory of convergence for algorithms producing inexact solutions of the sub problems, especially for trust regions methods~\cite{conn-etal:2000}. In a less formal way, it translates into a theoretical notion the practical aspect that computers are by nature inexact, because of floating-point arithmetic, and thus we are more interested into proving that "not exact but good enough" solutions will do the job. An algorithm that does not converge if solutions are not exact might be nice on paper but will not be very useful in practice.
	
	\section{The conjugate gradient method}
	
	In this section, we describe a very popular iterative method in optimization, the conjugate gradient (CG) method. We will try to give a comprehensive review and intuitive view of the method and its main characteristics. 
	
	\subsection{The main algorithm}
	
	Let $H \in \calS_n(\RR)$ such that $H \succ 0$, $c \in \RR^n$ and consider the quadratic program (QP)
	
	\begin{equation}\label{eq:unconstr_qp}
		\min_x \ q(x) = \dfrac{1}{2} \scal{x}{Hx} - \scal{c}{x}.
	\end{equation}
	
	This formulation will often occur in our case when we will formulate sub-problems from a quadratic model of the objective function.
	
	
	By positive-definiteness of $H$, program~\eqref{eq:unconstr_qp} is convex and thus has a unique minimizer $x^*$ verifying $\nabla q(x^*)=0$, i.e.
	\begin{equation}\label{eq:qp_linear_syst}
		Hx^* = c.
	\end{equation}
	Solving a convex QP is then equivalent to solving a symmetric linear system. The question is now how to solve it? A first approach could be to factorize $H$ to transform system~\eqref{eq:qp_linear_syst} into linear systems easier to solve, by using the Cholesky factorization or the QR decomposition for instance. See~\cite{golubvanloan:2013} for details on how to compute them. Using such direct methods can  still be prohibitive in computations and storage\footnote{Expand this paragraph to tell how exact methods can have issues}. The conjugate gradient method has the advantage to be iterative, to only require matrix-vector products and converges in at most, $n$ iterations. Before describing the principle of the method, we will introduce the notion of conjugate vectors w.r.t a symmetric matrix.
	
	\begin{definition}\label{def:conjugate_vectors}
		Let $H \succ 0$. Two vectors $u$ and $v$ are said to be $H$-conjugate if $\scal{u}{Hv}=0$.
	\end{definition} 
	
	Visually, two $H$-conjugate vectors are orthogonal in the space obtained obtained after applying $H$ to $\RR^n$. This definition naturally extends to a set of vectors $(p_i)_{i\in I}$ who are said to be $H$-conjugate if $\scal{p_i}{Hp_j}=0$ for all $(i,j) \in I^2$ such that $i\neq j$. If the matrix $H$ is implicit, we will just say "conjugate". 
	
	An interesting fact is that $k\le n$ conjugate vectors are also linearly independent. Thus, a set of $n$ conjugate vectors form a basis of $\RR^n$. Let's say that we initiate our iterative process from a given $x_0$ from which we seek to compute our solution $x^*$. If $(p_0,\ldots,p_{n-1})$ is a set of conjugate vectors, then there are scalars $(\alpha_0,\ldots,\alpha_{n-1})$ such that
	\[ x^*-x_0 = \sum_{i=0}^{n-1} \alpha_i p_i.\]
	
	Using the conjugacy property, one can find that the scalars are actually determined by the relation
	
	\[ \alpha_k = \dfrac{\scal{p_k}{H(x^*-x_0)}}{\scal{p_k}{Hp_k}}.\]
	
	The problem is that this relation depends on the solution that we are actually looking for and do not have access to\footnote{if we did, why solving the problem?}. However, this issue can be solved by introducing, for $k\ge 1$, the intermediates iterates 
	
	\[ x_k = x_0 + \sum_{i=0}^{k-1} \alpha_i p_i.\]
	
	The scalars can now be expressed
	
	\begin{equation}\label{eq:cg_steplength_1st_version}
		\alpha_k = -\dfrac{\scal{p_k}{r_k}}{\scal{p_k}{Hp_k}},
	\end{equation}
	
	where $r_k := Hx_k - c$ is the $k$-th residual. By the iterative relation $x_{k} = x_{k-1}+\alpha_{k-1}p_{k-1}$, one also has $r_{k}=r_{k-1} + \alpha_{k-1}Hp_{k-1}$.
	
	The latter relation can also be derived from performing a linesearch along the direction $p_k$ by computing $\alpha_k = \argmin_\alpha q(x_k + \alpha p_k)$. Since $q$ is quadratic, there is an explicit formula that also gives the relation~\eqref{eq:cg_steplength_1st_version}. Yet, we prefer to view this process as an iterative decomposition of the vector $x^*-x_0$ onto a basis of conjugate vectors. For instance, it is trivial that this process converges in $n$ iterations because each iteration consists in computing a coefficient of the decomposition. 
	
	In any case, the coefficients can now be computed recursively, provided we have access to conjugate directions, which is not a trivial question. If one knows its linear algebra classics, since $H$ is symmetric, by the spectral theorem, there is an orthonormal basis of eigen vectors into which $H$ is diagonal. With such vectors, the conjugacy property is clearly satisfied so this basis could do the job. The problem is that diagonalizing a matrix is a very expensive process on its own that could even exceed the cost of a relevant factorization of $H$. 
	
	Also note that having a base diagonalizing $H$ is stronger than what we actually need. Indeed, using matrix notations, let $P$ be the matrix whose columns are simply conjugate vectors $p_1,\ldots,p_n$ and $G \in O(n)$ such that $H=GDG^T$ for some diagonal matrix $D$. Then, because $G^TG=I_n$, one has $G^THG=D$, whereas definition~\ref{def:conjugate_vectors} simply implies that $P^THP$ is diagonal. The latter is weaker, because there is absolutely no reasons for $P$ to be in $O(n)$. Of course, $P$ is invertible but its inverse is not necessarily its transpose. 
	
	Another method would be to apply the Gram-Schmidt orthonormalization process to a set of given vectors. It would work but would still require storing all the vectors, which is less suited for large-scale applications. 
	
	Fortunately, there is a way to produce each direction iteratively in such a way that only accessible information about the problem and the previous basis vector are required to compute a new basis vector. The initial information will consist of the residuals $r_k$. Let $x_0$ be our starting point, $r_0=Hx_0-c$. For $k\ge 1$, every new basis vector is defined by
	
	\[p_k = -r_k + \beta_{k-1} p_{k-1}\]
	
	where $\beta_{k-1}$ is a conjugacy parameter whose expression can be derived from imposing $\scal{p_{k-1}}{Hp_k}=0$ and injecting the above expression:
	
	\[ \beta_{k-1} = \dfrac{\scal{p_{k-1}}{Hr_k}}{\scal{p_{k-1}}{Hp_{k-1}}}.\]
	
	Combined with the recursion previously described, this process respects our conditions to only require accessible information about the previous iteration and ensures the following properties:
	\begin{itemize}
		\item $\forall i<j,\ \scal{r_i}{r_j}=0$
		\item $\forall i\neq j,\ \scal{p_i}{Hp_j}=0$ ($H$-conjugacy).
		\item $\forall k,\ span(r_0,\ldots,r_k) = span(p_0,\ldots,p_k) = span(r_0,Hr_0,\ldots,H^{k-1}r_0)$
	\end{itemize}
	
	The space $span(r_0,Hr_0,\ldots,H^{k-1}r_0)$ is the Krylov subspace of degree $k$, generally written $\calK(H,r_0,k)$, is a the foundation of a category of iterative methods called Krylov methods, of which the CG method is a special instance. 
	
	Before describing the an algorithm, some relations can be modified such that the computations are even more efficient. First, with the conjugacy recursion, one has $\scal{p_k}{r_k} = - \|r_k\|^2$, leading to 
	
	\[\alpha_k = \dfrac{\|r_k\|^2}{\scal{p_k}{Hp_k}}.\]
	
	Similarly, the conjugacy parameter is also equal to 
	
	\[\beta_k = \dfrac{\|r_{k+1}\|^2}{\|r_k\|^2}.\]
	
	The CG method is stated in algorithm~\ref{algo:cg_method}.
	
	\begin{algorithm}
		\caption{The conjugate gradient method}\label{algo:cg_method}
		  
		\begin{algorithmic}
			\Require Starting point $x_0$
			\State $r_0 \gets Hx_0-c$ and $p_0 \gets -r_0$
			\For{$k=0,\ldots,n-2$ \textbf{or} until convergence}
				\State $\alpha_k \gets \frac{\|r_k\|^2}{\|p_k\|^2_H}$
				\State $x_{k+1} \gets x_k+\alpha_kp_k$
				\State $r_{k+1} \gets r_k + \alpha_kHp_k$
				\State $\beta_k =  \frac{\|r_{k+1}\|^2}{\|r_k\|^2}$
				\State $p_{k+1} \gets -r_{k+1}+\beta_kp_k$
			\EndFor{}
			\Return $x_k$
		\end{algorithmic}
	\end{algorithm} 
	
	 One can observe that each iteration only requires one matrix-vector product and inner products, making this method very efficient for large scale applications. To expand an earlier remark, the CG method belongs to the class of Krylov methods who consist in an iterative process for which, starting from $x_0$, each iterate is a minimizer of the objective function over a subspace of the form $\calK(H,r_0,k)$, also called a Krylov subspace. We have only seen an application for square symmetric systems but the theory extend to rectangular and indefinite systems.\footnote{TODO: add a reference to some Krylov methods}
	 
	 As we already said, the CG method converges in at most $n$ iterations, but the efficiency of the method is highly related to the eigen values distributions of the matrix.
	 
	 \begin{proposition}
	 	Let $H\in \calS_n^{++}(\RR)$ such that $H$ has $r<n$ distinct eigen values, then algorithm~\ref{algo:cg_method} applied to problem~\eqref{eq:unconstr_qp} converges in $r$ iterations. 
	 \end{proposition}
	 
	 In a similar fashion, if matrix $H$ has clustered eigen values, then the CG method will give a very good solution in a few iterations (as much as they are clusters). Then, by a change of coordinates, we could transform the linear system to get a matrix with clustered eigen values, which would lead to an acceleration of the efficiency of the method. This is the principle of preconditioning, that we will now discuss about.
	 
	 \subsection{Preconditioning}
	
	\section{Generalities on least squares}
	 
	 Rewriting the objective function of problem~\eqref{eq:model_cnls} as $f\colon x \mapsto  \frac{1}{2} \|r(x)\|^2$, one has:
	 
	 \begin{subequations}
	 		\begin{align}
	 		\nabla f(x) &= J(x)^Tr(x)\label{subeq:ls_grad} \\
	 		\nabla^2 f(x) &= J(x)^TJ(x) + S(x) , \label{subeq:ls_hessian}
	 		\end{align}
	 \end{subequations}
	 where $J(x) = \left[\dfrac{\partial r_i}{\partial x_j}\right]_{(i,j)}$ is the Jacobian matrix of the residuals and the second component of the Hessian $S(x) = \sum_{i=1}^{d} r_i(x) \nabla^2r_i(x) $. The latter is expensive in both computational time and storage, since it requires $d$ computations of $n\times n$ matrices. Hence, this component of the Hessian is the one to be approximated. 
	 
	 The Jacobian matrix of constraints function $h$ is noted $A$.
	 
	 When considering iterative methods for solving problem~\eqref{eq:model_cnls}, $k$ will, if not mentioned otherwise, refer to the iteration number. In order to simplify notations, quantities relative to a given iteration will be noted with the iteration number as an index, such as $x_k$ for the iterate, $r_k$ for $r(x_k)$ ,$J_k$ for $J(x_k)$ etc.
	 
	 Symbol $:=$ shall be used to state the definition of a numerical object (function, vector etc.)
	 
	 We now address is a quick review of the three most popular classes of approximations. For a comprehensive review of these methods, we refer the reader to the chapter 10 of \cite{dennisschnabel:1996}.
	 
	 \subsection{Gauss-Newton method}
	 
	 Originally used by Gauss the prince himself, \textbf{Gauss--Newton} (GN) approximation sets $S(x)$ to the zero matrix. It is the cheapest to compute, since the Jacobian is necessary to evaluate the gradient and works well in practice for zero residuals problems~\cite{dennisschnabel:1996}. When solving problem~\eqref{eq:model_cnls} using an iterative method, this approximation amounts to linearizing the residuals function within the norm. Indeed, approximating $\nabla f^2(x)$ by $J(x)^TJ(x)$ in a quadratic model $\calQ$ of $f$ around $x$ gives
	 
	 \begin{equation}\label{eq:gn_model}
	 	\calQ^{GN} (p) = \dfrac{1}{2}p^T\nabla f^2(x)p + \nabla f(x)^Tp =  \dfrac{1}{2} \left\Vert J(x)p+r(x) \right\Vert^2,
	 \end{equation}
	 
	 which corresponds to injecting the linearization $r(x+p) \approx J(x)p+r(x)$ in the squared norm.
	 
	 \subsection{Levenberg-Marquardt}
	 
	 Next, we describe the \textbf{Levengerg--Marquardt} (LM) method, respectively named after the first author to publish it~\cite{levenberg:1944} and the author of its best-known rediscovery~\cite{marquardt:1963}. As noted by Marquardt in his paper, the two authors started from a different line of reasoning but came to the same conclusion. In this method, matrix $S(x)$ is set to a multiple of the identity matrix $\sigma I$ where $\sigma$ is a positive scalar. The latter is called regularization parameter, because setting  $\nabla f^2(x)$ to $J(x)^TJ(x)$ leads to the quadratic model 
	 
	 \begin{equation}\label{eq:lm_model}
	 	\calQ^{LM}(p) =  \dfrac{1}{2} \left\Vert J(x)p+r(x) \right\Vert^2 + \dfrac{\sigma}{2} \|p\|^2.
	 \end{equation}
	 
	 In other words, one can see very schematically that
	 
	\[\text{LM model} = \text{GN model} + \text{regularization term}.\]
	
	In practice, the LM approximation works well on zero and small residuals problems and tends to be more robust than the GN approximation. It is still cheap to compute but only requires updating the regularization parameter throughout the iterative process. This method is often referred as the early stage of the trust region methods~\cite{conn-etal:2000}. For the link with regularization methods, consider the trust region problem
	 
	 \begin{equation*}
	 	\begin{aligned}
	 		\min_x \quad & f(x) \\
	 		\text{s.t.} \quad & \|x\| \le \Delta.
	 	\end{aligned}
	 \end{equation*}
	 
	 By applying KKT conditions and assuming that there is a minimizer $x^*$ lying on the trust region, i.e. $\|x^*\|=\Delta$, then (under strict complementarity), there is a strictly positive scalar (the multiplier) $\lambda$ such that
	 \[\nabla f(x^*) + \dfrac{2\lambda}{\Delta}x^*=0.\]
	 
	 As a consequence, $x^*$ is a critical point oh the unconstrained regularized problem
	 
	 \[\min_x \quad f(x) + \dfrac{\delta}{2} \|x\|^2,\]
	 
	 where $\delta=\frac{2\lambda}{\Delta}$.
	 
	 Some references for LM methods: \cite{bellavia-etal:2018}
	 
	 \subsection{Quasi-Newton}
	 
	 Finally, one can compute an approximation of $\nabla^2f(x)$ in a similar pattern as in \textbf{quasi-Newton} methods \cite[][Chapter 6]{nocedalwright:2006} but targeted on the second order components. It has a higher computational cost than the previous two but is more accurate on large residuals problems. In~\cite{dennisetal:1981}, the authors exploit this approach in a adaptative scheme, where an estimation of the curvature is used to decide whether or not the quasi-Newton approximation is worth to use compared to the Gauss-Newton one.
	 
	 It is important to bear in mind that choosing between a "cheap" approximation and a quasi-Newton type one implies making compromises. Depending on the initialization, the quasi-Newton approximation will take some iterations to be good and the accuracy will not be there when most needed, i.e. at the starting point potentially far from the solution, and will match the Gauss-Newton close to the solution on small residuals problems. In other words, the quasi-Newton is not accurate enough when most needed and very accurate when a way cheaper alternative does the same job.
	 
	 References for quasi-Newton specialized to least-squares: \cite{dennisetal:1981,yabetakahashi}.
	 
	 \section{Augmented Lagrangian reformulation}\label{sec:about_al}
	 
	 In this section, we introduce the framework of Augmented Lagrangian-based algorithms and describe an application in the least-squares setting of problem~\eqref{eq:model_cnls}.
	 
	 \subsection{Generalities}
	 
	 In order to remain general, we temporarily consider the mathematical program~\eqref{eq:math_prog} and shall comeback to our beloved least-squares shortly after. 
	 We introduce the Augmented Lagrangian (AL) function associated to program~\eqref{eq:math_prog}:
	 
	 \begin{equation}
	 	\label{eq:al}
	 	\Phi_A(x,\lambda,\mu) := f(x) + \scal{\lambda}{h(x)} + \dfrac{\mu}{2} \|h(x)\|^2,
	 \end{equation}
	 
	 where $\lambda\in \Real^m$ is the vector of Lagrange multipliers and $\mu > 0$ is the penalty parameter.
	 
	 Function~\eqref{eq:al} is nothing than the Lagrangian~\eqref{eq:lagrangian} with a quadratic penalty term, hence the adjective \textit{Augmented}.Assume we fixed $\lambda$ and $\mu$, the problem of interest is now the bound constrained program
	 
	 \begin{equation}\label{eq:al_nlp}
	 	\begin{aligned}
	 		\min_x \quad & \Phi_A(x,\lambda,\mu) \\
	 		\text{s.t.} \quad & l \le x \le u.
	 	\end{aligned}
	 \end{equation}
	 
	 AL-based algorithms fall into the class of penalty methods that generally enable one to use iterative methods for unconstrained optimization  while steel achieving feasibility. In our case, moving the nonlinear constraints into the objective simplifies the set of constraints since only bounds constraints are left.
	 
	 One of the pros of AL methods is that they come naturally with an update formula for the multipliers. Let \((x_k,\lambda_k)\) be the current primal-dual iterate, then one can choose \(\lambda_{k+1} \) as
	 
	 \begin{equation}\label{eq:al_multipliers_update}
	 	\lambda_{k+1} = \lambda_k + \mu_kh(x_k).
	 \end{equation}
	 
	 This relation is merely derived from \(\nabla_x\Phi_A=0\) by identifying the right hand side of equation~\eqref{eq:al_multipliers_update} as multipliers satisfying the KKT condition \(\nabla_x \ell =0\).
	 
	 The procedure of an AL method is relatively  and relies on solving successively  problem~\eqref{eq:al_nlp} with respect to the primal variable until a first order critical point of the original problem~\eqref{eq:math_prog} is found. At every iteration, the penalty parameter is increased and the multipliers are updated by formula~\eqref{eq:al_multipliers_update}. This general pattern is outlined in algorithm~\ref{algo:basic_al} and the main steps of the outer iteration are given in  algorithm~\ref{algo:outer_basic_al_trm}.
	 
	 	\begin{algorithm}
	 		\caption{Basic AL algorithm for solving~\eqref{eq:math_prog}}\label{algo:basic_al}
	 		\begin{algorithmic}
	 			\Require Starting point $\left(x_0^s,\lambda_0\right)$, parameter $\mu_0$  parameter $\mu_0$ and tolerances $\omega_*,\omega_0,\eta_*,\eta_0$.
	 			\Repeat
	 			 \State Compute an approximate solution $x_{k}$ of~\eqref{eq:al_nlp} starting from \(x_k^s\) with tolerance $\omega_k$.
	 			\If{\(\|h(x_k)\| < \eta_k\)}
	 				\State Update iterate and increase penalty parameter.
	 				\Else
	 					\State Restart minimization of~\eqref{eq:al_nlp} with a higher penalty parameter.
	 			\EndIf
	 			\State Update tolerances
	 			\Until{\(\|h(x_k)\| < \eta_*\) \textbf{and}  \(\|\nabla_x \ell(x_k,\lambda_k)\| < \omega_*\)}
	 			\State Return current approximate solution.
	 		\end{algorithmic}
	 	\end{algorithm}
	 	
	 	\begin{algorithm}
	 	\caption{Outer iteration of basic AL algorithm with trust region}\label{algo:outer_basic_al_trm}
	 		\begin{enumerate}
	 		\item[\textbf{Step 1: Inner Iteration}]
	 		
	 		Starting from $x_0^s$, approximately solve $\min_x \Phi_A(x,y_k,\mu_k)$ by computing $x_k$ such that \(\left\Vert x_k - P(x_k-\nabla_k\Phi_A)\right\Vert \le \omega_k\)
	 		by a trust region process.
	 		
	 		\textbf{If} $\|h(x_k)\| \le \eta_k$, execute \textbf{Step 2}.
	 		
	 		Otherwise, excecute \textbf{Step 3}.
	 		
	 		\item[\textbf{Step 2: Iterate Update}]
	 		
	 		Update $y_{k+1}$ by formula~\eqref{eq:al_multipliers_update} and set $x_{k+1}^s \gets x_k $.
	 		
	 		Choose new tolreances \(\omega_{k+1}, \eta_{k+1}\) and new penalty parameter \(\mu_{k+1}\).
	 		
	 		Increment $k$ and go back to \textbf{Step 1}.
	 		
	 		\item[\textbf{Step 3: Adjustment of the Penalty Parameter}] 
	 		
	 		Choose $\mu_{k+1}$ significantly greater than $\mu_k$.
	 		
	 		Leave the iterate unchanged: \(\left(x_{k+1}^s,\lambda_{k+1}\right) \gets \left(x_k^s,\lambda_k\right)\).
	 		
	 		Go back to \textbf{Step 1}.
	 	\end{enumerate}
	 \end{algorithm}
	 
	 \textbf{TODO How to compute the approximate minimizer? Projected conjugate gradient!}
	 \subsection{Application to structured least-squares}
	 
	 We now consider program~\eqref{eq:model_cnls}, for which the AL function is given by
	 
	 \begin{equation}
	 	\label{eq:lsal}
	 	\Phi_A(x,\lambda,\mu) := \dfrac{1}{2}\|r(x)\|^2 + \scal{\lambda}{h(x)} + \dfrac{\mu}{2} \|h(x)\|^2,
	 \end{equation}
	 
	 Contrary to formulation~\eqref{eq:al}, we keep the linear equality constraints as is and penalize the violation of the nonlinear constraints. Although the computation of the projection onto the set $\calX$ shall differ, the framework remains the same, 
	 One has the following expression of the gradient: 
	 \begin{equation}
	 	\label{eq:al_grad}
	 	\nabla_x \Phi_A(x,\lambda,\mu) = J(x)^Tr(x) + A^T\pi(x,\lambda,\mu),
	 \end{equation}
	 with $\pi(x,\lambda,\mu):=\lambda + \mu h(x)$ is the first-order estimates of the Lagrange multipliers. 
	 
	 The Hessian is given by
	 \begin{equation}\label{eq:al_hessian}
	 	\nabla^2_{xx} \Phi_A(x,\lambda,\mu) = J(x)^TJ(x) + \mu A(x)^TA(x) +  S(x) + \sum_{i=1}^d \nabla^2 h_i(x) \pi(x,\lambda,\mu).
	 \end{equation}
	 
	 For fixed $\lambda$ and $\mu$, reformulating problem~\eqref{eq:model_cnls} with function~\eqref{eq:al} gives the linearly constrained problem
	 
	 \begin{equation}\label{eq:model_cnls_al_reformulation} 
	 	\begin{aligned}
	 		\min_{x} \quad& \Phi_A(x,\lambda,\mu)  \\
	 		\text{s.t.}  \quad & x \in \calX 
	 	\end{aligned}	
	 \end{equation}
	 
	 As for any other AL based algorithm, the idea behind MCWAL is to solve by an iterative method problem~\eqref{eq:model_cnls_al_reformulation} until a first order critical point of problem~\eqref{eq:model_cnls} is found. At every iteration, a the new iterate will be computed after (approximately) solving a trust region subproblem formed after a quadratic model of the AL around the current iterate. 
	 
	 \subsection{Subproblem}
	 
	 Given a primal-dual iterate $(x_k,\lambda_k)$ and a penalty parameter $\mu_k$, we consider a quadratic model of the AL around $x_k$:
	 
	 \begin{equation}\label{eq:quadratic_al}
	 	\calQ_k(p) = \dfrac{1}{2}\scal{p}{H_kp} + \scal{g_k}{p},
	 \end{equation}
	 
	 where $H_k:=\nabla^2_{xx} \Phi_A(x_k,\lambda_k,\mu_k)$ or an approximation of it and $g_k:=\nabla_x \Phi_A(x_k,\lambda_k,\mu_k)$.
	 
	 Vector $p$ denotes the unknown of the subproblem whose (approximate) solution $p_k$ shall be used to compute the new iterate $x_{k+1}=x_k+p_k$.
	 
	 For the linear constraints, we want $x_k+p\in \calX$ which will be provided if:
	 \begin{itemize}
	 	\item \(Cp=0\) (provided that $Cx_0=b$)
	 	\item$ x_k-l \le p \le u-x_k$
	 \end{itemize}
	 The above conditions shall be written $p\in \calX_k$ where $\calX_k:= \left\{p \ | \ Cp=0,\ x_k-l \le p \le u-x_k\right\}$.
	 
	As part of our method, we will also add a trust region constraint of the form $\|p\|_k \le \Delta_k$ for a radius $\Delta_k$ and a norm $\|\cdot\|_k$. Index $k$ in the latter means that the norm might depend on the iteration. A priori, we would use the euclidean norm.
	
	The subproblem of an outer iteration is then given by
	
	\begin{equation}\label{eq:quadratic_subpb} 
		\begin{aligned}
			\min_{p\in \calX_k} \quad& \calQ_k(p)  \\
			\text{s.t.}  \quad &  \|p\|_k \le \Delta_k.
		\end{aligned}	
	\end{equation}
	
	A first sketch of the MCWAL procedure is drawn in algorithm~\ref{algo:sketch_mcwal}.
	 \begin{algorithm}
	 	\caption{Sketch of MCWAL}\label{algo:sketch_mcwal}
	 	\begin{algorithmic}
	 		\Require $x_0\in \calX$, $\lambda_0$, $\mu_0, \tau_0$ and constants $\eta_s$
	 		\While{\textbf{not optimal}\footnote{replace with a numerical criteria corresponding to KKT condition}}
	 		\State Evaluate $H_k$ and $g_k$ 
	 		\State Compute a solution $p_k$ of subproblem~\eqref{eq:quadratic_subpb} 
	 		\State Compute ratio $\rho_k$\footnote{TODO: define its expression}
	 		\If{$\rho_k \ge \eta_s$} \Comment{Good step}
	 		\State $x_{k+1} \gets x_k+p_k$
	 		\State Choose $\Delta_{k+1} > \Delta_k$
	 		\If{$\|h(x_k)\| \le \tau_k$}
	 		\State $y_{k+1} \gets \pi(x_k,y_k,\mu_k)$
	 		\State Choose $\mu_{k+1} > \mu_k$ and $\tau_{k+1} < \tau_k$
	 		\Else
	 		\State $y_{k+1} \gets y_k$
	 		\State Choose $\mu_{k+1} < \mu_k$
	 		\EndIf
	 		\Else \Comment{Bad step}
	 		\State  $x_{k+1} \gets x_k$
	 		\State  $y_{k+1} \gets y_k$
	 		\State  $\mu_{k+1} \gets \mu_k$
	 		\State Choose $\Delta_{k+1} < \Delta_k$
	 		\EndIf
	 		\EndWhile
	 	\end{algorithmic}
	 \end{algorithm}
	

	\clearpage
	
	\bibliographystyle{plainnat}
	\bibliography{refs}
\end{document}