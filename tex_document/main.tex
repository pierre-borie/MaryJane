%%% ArXiv template from https://www.overleaf.com/latex/templates/an-arxiv-template/gbzmznbxvwpr

\documentclass[reqno]{amsart}
\usepackage{graphicx}
\baselineskip=16pt

\usepackage{indentfirst,csquotes}

\topmargin= .5cm
\textheight= 20cm
\textwidth= 32cc
\baselineskip=16pt

\evensidemargin= .9cm
\oddsidemargin= .9cm

\usepackage{amssymb,amsthm,amsmath}
\usepackage{xcolor,paralist,hyperref,fancyhdr,etoolbox}


\newtheorem{theorem}{Theorem}[]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}


\hypersetup{colorlinks=true, linkcolor=black, filecolor=black, urlcolor=black }
\def\proof{\noindent {\it Proof. $\, $}}
\def\endproof{\hfill $\Box$ \vskip 5 pt }









%%% PERSONAL ADD-ONS
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\allowdisplaybreaks
\numberwithin{equation}{section}
%\usepackage{natbib}
\include{macros}

\begin{document}
	
	
	\title{Ongoing work on MCWAL} %%%%%%%%%%%%
	\author{Pierre Borie}
	\address{Deparment of Computer Science and Operations Research, University of Montreal, Montreal, QC, Canada.}
	\date{\today}
	
	\begin{abstract}
		\noindent This informal document  reflects the ongoing work and thinking on a algorithm for constrained nonlinear least squares. The current algorithm (rapper) name is MCWAL for Moindres Carr\'es With Augmented Lagrangian.
	\end{abstract} %%%%%%%%% 
	
	\maketitle
	
	\let\thefootnote\relax

	\section{Introduction}\label{sec:intro}
	
	We consider least squares problems subject to both nonlinear and linear constraints of the form
	\begin{equation}
		\label{eq:model_cnls}
		\begin{aligned}
			\min_{x\in \RR^n} \quad & \dfrac{1}{2} \|r(x)\|^2 \\
			\text{s.t.} \quad & h(x) = 0 \\
			& \scal{c_i}{x} = b_i,\quad i=1,\ldots,m \\
			& \ell \le x \le u,
		\end{aligned}
	\end{equation}
	where $r\colon \RR^n \to \RR^d$  and $h\colon \RR^n \to \RR^t$ are assumed to be nonlinear, potentially non convex, continuously differentiable functions, $\scal{\cdot}{\cdot}$ is the canonical inner product and $\|\cdot\| $  its induced euclidean norm, $c_i$ are $m$  independent vectors of $\Real^n$, $( m \le n)$, $b=(b_1,\ldots,b_m)^T \in \RR^m$ and $\ell$ and $u$ are vectors in $\RR^n$. Without loss of generality, some components of the latter two vectors can be set to $\pm \infty$ for unbounded parameters. In the context of least squares problems, components $r_i$ of the function $r$ are often denoted as the residuals.
	
	We will also refer to the linear constraints using the following set notation 
	
	\begin{equation}
		\label{eq:linear_constraints}
		\calX = \left\{ x \in \RR^n \ | \ Cx=b,\ \ell \le x \le u\right\},
	\end{equation}
	where $C$ is the matrix whose columns are the vectors $c_i$. By linear independence of those vectors, $C$ is a full rank matrix. The set $\calX$ is thus convex.
	
	\subsection{Notations and reminders of nonlinear programming}
	
	The Jacobian matrix of constraints function $h$ is noted $A$.
	
	When considering iterative methods for solving problem~\eqref{eq:model_cnls}, $k$ will, if not mentioned otherwise, refer to the iteration number. In order to simplify notations, quantities relative to a given iteration will be noted with the iteration number as an index, such as $x_k$ for the iterate, $r_k$ for $r(x_k)$ ,$J_k$ for $J(x_k)$ etc.
	
	Symbol $:=$ shall be used to state the definition of a numerical object (function, vector etc.)
	
	\subsection{Generalities on least squares}
	 
	 Rewriting the objective function of problem~\eqref{eq:model_cnls} as $f\colon x \mapsto  \frac{1}{2} \|r(x)\|^2$, one has:
	 
	 \begin{subequations}
	 		\begin{align}
	 		\nabla f(x) &= J(x)^Tr(x)\label{subeq:ls_grad} \\
	 		\nabla^2 f(x) &= J(x)^TJ(x) + S(x) , \label{subeq:ls_hessian}
	 		\end{align}
	 \end{subequations}
	 where $J(x) = \left[\dfrac{\partial r_i}{\partial x_j}\right]_{(i,j)}$ is the Jacobian matrix of the residuals and the second component of the Hessian $S(x) = \sum_{i=1}^{d} r_i(x) \nabla^2r_i(x) $. The latter is expensive in both computational time and storage, since it requires $d$ computations of $n\times n$ matrices. Hence, this component of the Hessian is the one to be approximated. 
	 
	 The Jacobian matrix of constraints function $h$ is noted $A$.
	 
	 When considering iterative methods for solving problem~\eqref{eq:model_cnls}, $k$ will, if not mentioned otherwise, refer to the iteration number. In order to simplify notations, quantities relative to a given iteration will be noted with the iteration number as an index, such as $x_k$ for the iterate, $r_k$ for $r(x_k)$ ,$J_k$ for $J(x_k)$ etc.
	 
	 Symbol $:=$ shall be used to state the definition of a numerical object (function, vector etc.)
	 
	 We now address is a quick review of the three most popular classes of approximations. For a comprehensive review of these reviews, we refer the reader to the chapter 10 of \cite{dennisschnabel:1996}.
	 
	 The \textbf{Gauss--Newton} approximation sets $S(x)$ to the zero matrix. It is the cheapest to compute, since the Jacobian is necessary to evaluate the gradient and works well in practice for zero residuals problems~\cite{dennisschnabel:1996}. When solving problem~\eqref{eq:model_cnls} using an iterative method, this approximation amounts to linearizing the residuals function within the norm.
	 
	 Next, the \textbf{Levengerg--Marquardt} (LM) method~\cite{levenberg:1944,marquardt:1963} sets $S(x)$ to a multiple of the identity matrix $\sigma I$ where $\sigma$ is a regularization parameter. It is still cheap to compute but only requires updating the regularization parameter throughout the iterative process. This method can be seen as the early stage of the trust region methods~\cite{conn-etal:2000}. It works well in practice on zero and small residuals problems. 
	 
	 Finally, one can compute an approximation of $\nabla^2f(x)$ in a similar pattern as in \textbf{quasi-Newton} methods \cite[][Chapter 6]{nocedalwright:2006} but targeted on the second order components. It has a higher computational cost than the previous two but is more accurate on large residuals problems. In~\cite{dennisetal:1981}, the authors exploit this approach in a adaptative scheme, where an estimation of the curvature is used to decide whether or not the quasi-Newton approximation is worth to use compared to the Gauss-Newton one.
	 
	 It is important to bear in mind that choosing between a "cheap" approximation and a quasi-Newton type one implies making compromises. Depending on the initialization, the quasi-Newton approximation will take some iterations to be good and the accuracy will not be there when most needed, i.e. at the starting point potentially far from the solution, and will match the Gauss-Newton close to the solution on small residuals problems. In other words, the quasi-Newton is not accurate enough when most needed and very accurate when a way cheaper alternative does the same job.
	 
	 
	 
	 \section{Augmented Lagrangian reformulation}
	 
	 We introduce the Augmented Lagrangian (AL) where only the violation of the nonlinear constraints is penalized:
	 
	 \begin{equation}
	 	\label{eq:al}
	 	\calL_A(x,\lambda,\mu) := \dfrac{1}{2}\|r(x)\|^2 + \scal{\lambda}{h(x)} + \dfrac{\mu}{2} \|h(x)\|^2,
	 \end{equation}
	 
	 where $\lambda\in \Real^m$ is the vector of Lagrange multipliers and $\mu > 0$ is the penalty parameter.
	 
	 One has the following expression of the gradient: 
	 \begin{equation}
	 	\label{eq:al_grad}
	 	\nabla_x \calL_A(x,\lambda,\mu) = J(x)^Tr(x) + A^T\pi(x,\lambda,\mu),
	 \end{equation}
	 with $\pi(x,\lambda,\mu):=\lambda + \mu h(x)$ is the first-order estimates of the Lagrange multipliers. 
	 
	 The Hessian is given by
	 \begin{equation}\label{eq:al_hessian}
	 	\nabla^2_{xx} \calL_A(x,\lambda,\mu) = J(x)^TJ(x) + \mu A(x)^TA(x) +  S(x) + \sum_{i=1}^d \nabla^2 h_i(x) \pi(x,\lambda,\mu).
	 \end{equation}
	 
	 For fixed $\lambda$ and $\mu$, reformulating problem~\eqref{eq:model_cnls} with function~\eqref{eq:al} gives the linearly constrained problem
	 
	 \begin{equation}\label{eq:model_cnls_al_reformulation} 
	 	\begin{aligned}
	 		\min_{x} \quad& \calL_A(x,\lambda,\mu)  \\
	 		\text{s.t.}  \quad & x \in \calX 
	 	\end{aligned}	
	 \end{equation}
	 
	 As for any other AL based algorithm, the idea behind MCWAL is to solve by an iterative method problem~\eqref{eq:model_cnls_al_reformulation} until a first order critical point of problem~\eqref{eq:model_cnls} is found. At every iteration, a the new iterate will be computed after (approximately) solving a trust region subproblem formed after a quadratic model of the AL around the current iterate. 
	 
	 \subsection{Subproblem}
	 
	 Given a primal-dual iterate $(x_k,\lambda_k)$ and a penalty parameter $\mu_k$, we consider a quadratic model of the AL around $x_k$:
	 
	 \begin{equation}\label{eq:quadratic_al}
	 	\calQ_k(p) = \dfrac{1}{2}\scal{p}{H_kp} + \scal{g_k}{p},
	 \end{equation}
	 
	 where $H_k:=\nabla^2_{xx} \calL_A(x_k,\lambda_k,\mu_k)$ or an approximation of it and $g_k:=\nabla_x \calL_A(x_k,\lambda_k,\mu_k)$.
	 
	 Vector $p$ denotes the unknown of the subproblem whose (approximate) solution $p_k$ shall be used to compute the new iterate $x_{k+1}=x_k+p_k$.
	 
	 For the linear constraints, we want $x_k+p\in \calX$ which will be provided if:
	 \begin{itemize}
	 	\item \(Cp=0\) (provided that $Cx_0=b$)
	 	\item$ x_k-\ell \le p \le u-x_k$
	 \end{itemize}
	 The above conditions shall be written $p\in \calX_k$ where $\calX_k:= \left\{p \ | \ Cp=0,\ x_k-\ell \le p \le u-x_k\right\}$.
	 
	As part of our method, we will also add a trust region constraint of the form $\|p\|_k \le \Delta_k$ for a radius $\Delta_k$ and a norm $\|\cdot\|_k$. Index $k$ in the latter means that the norm might depend on the iteration. A priori, we would use the euclidean norm.
	
	The subproblem of an outer iteration is then given by
	
	\begin{equation}\label{eq:quadratic_subpb} 
		\begin{aligned}
			\min_{p\in \calX_k} \quad& \calQ_k(p)  \\
			\text{s.t.}  \quad &  \|p\|_k \le \Delta_k.
		\end{aligned}	
	\end{equation}
	
	A first sketch of the MCWAL procedure is drawn in algorithm~\ref{algo:sketch_mcwal}.
	 \begin{algorithm}
	 	\caption{Sketch of MCWAL}\label{algo:sketch_mcwal}
	 	\begin{algorithmic}
	 		\Require $x_0\in \calX$, $\lambda_0$, $\mu_0, \tau_0$ and constants $\eta_s$
	 		\While{\textbf{not optimal}\footnote{replace with a numerical criteria corresponding to KKT condition}}
	 		\State Evaluate $H_k$ and $g_k$ 
	 		\State Compute a solution $p_k$ of subproblem~\eqref{eq:quadratic_subpb} 
	 		\State Compute ratio $\rho_k$\footnote{TODO: define its expression}
	 		\If{$\rho_k \ge \eta_s$} \Comment{Good step}
	 		\State $x_{k+1} \gets x_k+p_k$
	 		\State Choose $\Delta_{k+1} > \Delta_k$
	 		\If{$\|h(x_k)\| \le \tau_k$}
	 		\State $y_{k+1} \gets \pi(x_k,y_k,\mu_k)$
	 		\State Choose $\mu_{k+1} > \mu_k$ and $\tau_{k+1} < \tau_k$
	 		\Else
	 		\State $y_{k+1} \gets y_k$
	 		\State Choose $\mu_{k+1} < \mu_k$
	 		\EndIf
	 		\Else \Comment{Bad step}
	 		\State  $x_{k+1} \gets x_k$
	 		\State  $y_{k+1} \gets y_k$
	 		\State  $\mu_{k+1} \gets \mu_k$
	 		\State Choose $\Delta_{k+1} < \Delta_k$
	 		\EndIf
	 		\EndWhile
	 	\end{algorithmic}
	 \end{algorithm}
	
	\clearpage
	
	\bibliographystyle{plainnat}
	\bibliography{refs}
\end{document}