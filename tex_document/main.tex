%%% ArXiv template from https://www.overleaf.com/latex/templates/an-arxiv-template/gbzmznbxvwpr

\documentclass[10pt]{article}
\usepackage{graphicx}
\baselineskip=16pt

\usepackage{indentfirst,csquotes}

\topmargin= .5cm
\textheight= 20cm
\textwidth= 32cc
\baselineskip=16pt

\evensidemargin= .9cm
\oddsidemargin= .9cm

\usepackage{amssymb,amsthm,amsmath}
\usepackage{xcolor,paralist,hyperref,fancyhdr,etoolbox}


\newtheorem{theorem}{Theorem}[]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}


\hypersetup{colorlinks=true, linkcolor=black, filecolor=black, urlcolor=black }
\def\proof{\noindent {\it Proof. $\, $}}
\def\endproof{\hfill $\Box$ \vskip 5 pt }









%%% PERSONAL ADD-ONS
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\allowdisplaybreaks
\numberwithin{equation}{section}
%\usepackage{natbib}

% MACROS
\newcommand{\scal}[2]{\left\langle {#1} , {#2} \right\rangle} % inner product
\include{macros}

% For foot notes in author name
\newcommand{\footremember}[2]{%
	\footnote{#2}
	\newcounter{#1}
	\setcounter{#1}{\value{footnote}}%
}
\newcommand{\footrecall}[1]{%
	\footnotemark[\value{#1}]%
} 

\begin{document}
	
	
	\title{Ongoing work on TRALCNLLSS} %%%%%%%%%%%%
	\author{Pierre Borie\footremember{1}{University of Montreal, Department of Computer Science and Operations Research, Montreal, QC, Canada}}
	\date{}
	
	
	
	\maketitle
	\tableofcontents
	
	\begin{abstract}
		\noindent This informal document deals with the ongoing work and thinking on a algorithm for constrained nonlinear least squares. The current algorithm  name is TRALCNLLSS for Trust Region Augmented Lagrangian Constrained Nonlinear Least-Squares Solver. Some sections about more general aspects about nonlinear programming might also appear here and there. The aim of this document is to reflect the thinking and understanding of different kinds of notions. 
	\end{abstract} %%%%%%%%% 
	

	\section{Introduction}\label{sec:intro}
	
	We consider least squares problems subject to both nonlinear and linear constraints of the form
	\begin{equation}
		\label{eq:model_cnls}
		\begin{aligned}
			\min_{x\in \RR^n} \quad & \dfrac{1}{2} \|r(x)\|^2 \\
			\text{s.t.} \quad & h(x) = 0 \\
			& \scal{c_i}{x} = b_i,\quad i=1,\ldots,m \\
			& l \le x \le u,
		\end{aligned}
	\end{equation}
	where $r\colon \RR^n \to \RR^d$  and $h\colon \RR^n \to \RR^t$ are assumed to be nonlinear, potentially non convex, continuously differentiable functions, $\scal{\cdot}{\cdot}$ is the canonical inner product and $\|\cdot\| $  its induced euclidean norm, $c_i$ are $m$  independent vectors of $\Real^n$, $( m \le n)$, $b=(b_1,\ldots,b_m)^T \in \RR^m$ and $\ell$ and $u$ are vectors in $\RR^n$. Without loss of generality, some components of the latter two vectors can be set to $\pm \infty$ for unbounded parameters. In the context of least squares problems, components $r_i$ of the function $r$ are often denoted as the residuals.
	
	We will also refer to the linear constraints using the following set notation 
	
	\begin{equation}
		\label{eq:linear_constraints}
		\calX = \left\{ x \in \RR^n \ | \ Cx=b,\ l \le x \le u\right\},
	\end{equation}
	where $C$ is the matrix whose columns are the vectors $c_i$. By linear independence of those vectors, $C$ is a full rank matrix. The set $\calX$ is thus convex.
	
	\section*{Notations}
	 
	
	When considering iterative methods for solving problem~\eqref{eq:model_cnls}, $k$ will, if not mentioned otherwise, refer to the iteration number. In order to simplify notations, quantities relative to a given iteration will be noted with the iteration number as an index, such as $x_k$ for the iterate, $f_k$ for the evaluation $f(x_k)$ etc.
	
	Symbol $:=$ shall be used to state the definition of a numerical object (function, vector etc.)
	
	For a subset $C$ of a metric space $E$, when considering a sequence $(x_k)_k \in E^{\NN}$ converging to $x^* \in C$ when $k\to+\infty$, we will write $x_k \to_E x^*$ if, above a certain index, all $x_k$ are in $C$. 
	
	The set of linear combinations of a set of vectors $(v_1,\ldots,v_n)$ is written $\rangle$ $span(v_1,\ldots,v_n)$. 
	
	$\calM_{m,n}(\bbK)$ denotes the set of matrices with $m$ rows and $n$ columns whose coefficients are in the field $\bbK = \RR$ or $\CC$. In practice, the field $\bbK$ will always be $\RR$ and it would be surprising to consider the complex case but one never knows. When $m=n$, i.e. for square matrices, we will write $\calM_n(\bbK)$. The space of one columns matrices $\calM_{n,1}$ shall be identified with $\bbK^n$.. The adjoint of a matrix $A\in \calM_{m,n}(\bbK)$ is written $A^*$. In the real case, it corresponds to its transpose, in the complex case, it corresponds to the transpose of its conjugate. 
	
	We also introduce notations for special subsets of matrices. $GL_n(\bbK)$, also known as the linear group of order $n$, denotes the set of invertible matrices (or "non-singular"...). $O(n)$ is the orthogonal group, i.e. the set of matrices $Q$ in $\calM_n(\RR)$ such that $Q^TQ=I_n$. $\calS_n(\bbK)$ denotes the set of matrices equal to their adjoint. In the real case, such matrices are called symmetric, in the complex case, they are called hermitian.  
	
	The condition number of a matrix $A\in \calM_n(\bbR)$, given by $\|A\|\ \|A^{-1}\|$ will be noted $\kappa(A)$. 
	
	If $H$ is a symmetric matrix, i.e. $H^T=H$, we will write $H \succ 0$, resp. $H \succeq 0$, when $H$ is positive definite, resp. semi-definite positive. In the case $H\succ 0$, the bilinear mapping $\scal{\cdot}{\cdot}_H:(x,y) \mapsto \scal{x}{Hy} $ forms an inner product on $\RR^n$ of induced norm $\|\cdot\|_H:x\mapsto \sqrt{\scal{x}{Hx}}$.
	
	
	\section{Optimality conditions for nonlinear programming}\label{sec:nlp_optimality_conditions}
	
	In this section, we describe optimality conditions for nonlinear programs more general than~\eqref{eq:model_cnls} from different views.
	
	\subsection{An algebraic view}
	
	We consider the general mathematical program\footnote{Shall I write the KKT conditions w.r.t. this formulation and then consider multipliers for the bounds?}
	\begin{equation}
		\label{eq:math_prog}
		\begin{aligned}
			\min_x \quad & f(x) \\
			\text{s.t.} \quad & h(x)=0 \\
			& l \le x \le u,
		\end{aligned}
	\end{equation}
	with the same assumptions on of differentiable of functions $f$ and \(h\). We introduce the Lagrangian associated to problem~\eqref{eq:math_prog}:
	\begin{equation}
		\label{eq:lagrangian}
		\ell(x,\lambda) = f(x) + \scal{\lambda}{h(x)},
	\end{equation}
	where $\lambda$ is the vector of Lagrange multipliers. Algorithms discussed aim to find local minimum of problem~\eqref{eq:math_prog}, i.e. feasible and of minimal value in s neighborhood. Under suitable assumptions, one can establish necessary and even sufficient conditions for local optimality. One of the most important assumptions belongs to the familty of constraints qualifications. The following is the one we will employ.
	
	\begin{definition}\label{def:licq}
		\textbf{(LICQ)}
		The LICQ holds at $x^*$ if the gradients of the equality constraints evaluated at $x^*$ are linearly independent. In other terms, the matrix $A(x)$ is full rank.
	\end{definition}
	
	Using this and standard differentiability assumptions, one can state first-order necessary optimality conditions, also known as KKT conditions. 
	
	\begin{definition}\label{def:kkt_point}
		\textbf{(KKT conditions)}
		A point $x^*$ satisfies the KKT conditions if $x^* $ is feasible and there exists multipliers \(\lambda^*\) such that \[\nabla_x \ell (x^*,\lambda^*)=0.\]
	\end{definition}
	
	
	A point satisfying those conditions if also said to be a KKT point or a first order critical point for problem~\eqref{eq:math_prog}. Depending on the context, we would refer to a KKT point either by just wrting $x^*$ or the couple formed after $x^*$ and its associated Lagrange multiplier $\lambda^*$The necessary conditions follow.
	
	\begin{theorem}\label{theo:fonc}
		\textbf{(First Order Necessary Conditions)\cite[][Theorem 12.1]{nocedalwright:2006}}
		Let $x^*$ be a local solution of~\eqref{eq:math_prog} at which the LICQ holds. Then $x^*$ is a KKT point.
	\end{theorem}
	
	Sufficient optimality conditions can be established using second order information.
	
	\begin{definition}\label{def:soc}
		\textbf{(Second Order Conditions)}
		A point $(x^*,\lambda^*)$ satisfies the second order conditions if it is a KKT point for ~\eqref{eq:math_prog} at which the LICQ holds and if the matrix $\nabla_{xx}^2\ell(x^*,\lambda^*)$ is positive definite on the null space of the constraints Jacobian, i.e.:
		\[\forall w \text{ verifying } \scal{A(x^*)}{w}=0,\ \scal{w}{\nabla_{xx}^2\ell(x^*,\lambda^*)w} > 0.\]
	\end{definition}
	
	\begin{theorem} \textbf{(Second Order Sufficient Conditions) \cite[][Theorem 12.5]{nocedalwright:2006}}
		If $x^*$ satisfies the second order conditions, then $x^*$ is a local minimum of~\eqref{eq:math_prog}.
	\end{theorem}
	
	\subsection{A geometric view}
	
	We now consider the general program
	
	\begin{equation}
		\label{eq:math_prog_geom}
		\begin{aligned}
			\min_x \quad & f(x) \\
			\text{s.t.} \quad & x \in \calC,
		\end{aligned}
	\end{equation}
	
	where the feasible set $\calC$ is a (of course non empty), subset of $\RR^n$. The idea is to describe optimality conditions using inclusions of certain vectors to sets and cones. General results can be stated without any further assumptions on $\calC$. Yet, we will give important results about projections that require to assume $\calC$ is convex. As we shall see in section~\ref{sec:about_al}, our method will imply solving quadratic programs subject to linear, hence convex, constraints. Most of the content exposed is from~\cite[][Chapter 12]{nocedalwright:2006}.
	
	We start by defining two important sets, that are actually cones. We remind that a set $K$ is a cone if for all $x \in K$ and $\alpha >0 $, $\alpha x\in K $. A cone is pointed if it contains $0$.
	
	\begin{definition}\textbf{Tangent Vector}
		Let $x\in \calC$. A vector $v\in \RR^n$ is said to be a tangent vector to $\calC$  at $x$ if there are sequences $(x_k)_k \to_\calC x$ and $(t_k)_k \searrow 0^+$ such that 
		\[\lim\limits_{k\to \infty} \dfrac{x_k-x}{t_k}=v.\]
	\end{definition}
	
	\begin{definition}
		
	The \textbf{tangent cone} is the set of all tangents vectors to $\calC$ at $x$ and is denoted by $T_\calC(x)$.
	\end{definition}
	
	For a better understanding, the limit in the definition can be written as the relation
	\[x_k - x = t_kv + o(1).\]
	Intuitively, tangent vectors are all the directions going from $x$ that stay in $\calC$, up to a scaling by a very small positive factor. In the context of constrained optimization, they characterize directions in which one can step away from $x$ while remaining feasible. 
	
	\textbf{Remark} 
	
	Rigorously, it is a $o(t_k)$ but since $t_k \to 0$, a $o(t_k)$ is also a $o(1)$. Our intent is to highlight the  visualization of a tangent vector as a scaled $x_k-x$ for $(x_k)_k \to x$.
	
	\begin{definition}
		
		The \textbf{normal cone} to $\calC$ at $x$ is the set defined by
		\[N_\calC (x) := \left\{w \in \RR^n\ | \ \forall v \in T_\calC(x), \scal{v}{w} \le 0\right\}.\]
	\end{definition}
	
	The normal cone is merely the orthogonal complement of the tangent cone and is involved in the following first order optimality condition.
	
	\begin{theorem}\label{theo:first_order_normal_cone}
		If $x^*$ is a local minimum of $f$, then \[-\nabla f(x^*) \in N_\calC(x^*).\]
	\end{theorem}
	
	This theorem reflects the intuitive fact that a local minimum is a point from which the objective function cannot be reduced while remaining feasible. While this result is simple is elegant, its use is limited by the difficulty to express both tangent and normal cones without any other assumptions. For instance, with the LICQ, the tangent cone can be expressed as an intersection of hyperplanes depending on the constraints gradients and combining theorem~\ref{theo:first_order_normal_cone} with a Farkas-like lemma gives the KKT conditions. Similar consequences occur when the feasible set is convex, which we will assume from now on.
	
	We first remind the definition a convex set.
	
	\begin{definition}
		A set $\calC\subseteq \RR^n$ is \textbf{convex} if \(\forall (x,y) \in \calC^2\) and \(\ \alpha \in [0,1] \), one has \(\alpha x+ (1-\alpha)y \in \calC\).
		In other words, the line segment between two points remains in $\calC$, for all points of $\calC$.
	\end{definition}
	
	In this setting, the normal cone can be described more simply.
	
	\begin{proposition}\label{prop:normal_set_convex}
		Let $\calC $ a non empty convex set. Then for all $x \in \calC$:
		\[N_\calC (x) = \left\{v \ | \ \forall y \in \calC, \scal{v}{x-y} \ge 0\right\}.\]
		For $x \notin \calC$, $N_\calC (x) = \emptyset$.
	\end{proposition}
	
	Then, if $x^*$ is a local minima of~\eqref{eq:math_prog_geom}, the inclusion in theorem~\ref{theo:first_order_normal_cone}  becomes
	\begin{equation}\label{eq:fo_optimality_convex_set}
		\forall x \in \calC, \scal{\nabla f(x^*)}{x-x^*} \ge 0.
	\end{equation}
	
	\textbf{Remarks on~\eqref{eq:fo_optimality_convex_set}}
	\begin{enumerate}
		\item This condition if sufficient when the function $f$ is convex
		\item It also reflects the very visual fact that in $\RR^2$, the line tangent to a local minima is below the curve
	\end{enumerate}
	
	So we now have a more simple way to characterize a minimum but it is still not suited to be used in algorithms. Obviously, one cannot check the sign of an infinite number of inner product but more fundamentally, algorithm must be able to manipulate points that are likely to be outside of the feasible set. Using projections is the key.
	
	\begin{definition}\label{def:projection}
		
		Let $\calC \subseteq \RR^n$. The \textbf{projection operator} is the set-value mapping $P_\calC \colon \RR^n \rightrightarrows \calC $ defined by
		\[ P_C(\bar{x}) = \argmin_{y \in \calC}  \dfrac{1}{2} \|\bar{x}-y\|^2.\]
	\end{definition}
	
	For this definition, we introduce the notion of set-value mapping~\cite{rockafellarwets:1998} to remain general but in the convex case, it is single valued and well defined if the concerned set is closed. 
	
	\begin{theorem}
		Let $\calC$ be a close convex set and $x \in \RR^n$. Then
		\[\bar{x} = P_\calC(x) \iff \forall y \in \calC, \scal{\bar{x}-x}{y-\bar{x} \ge 0.}\]
	\end{theorem}
	
	Since the norm is convex, this theorem is simply derived from the first order necessary optimality  conditions. Also the inequality kind of looks like the inequality in proposition~\eqref{prop:normal_set_convex}, which motivates the following theorem.
	
	\begin{theorem}
		Let $\calC$ be a close convex set, $x \in \RR^n$ and $z \in \calC$. Then
		\[z = P_\calC(x) \iff (x-z) \in N_\calC(z).\]
	\end{theorem}
	
	From this result, one deduces the following useful corollary.
	
	\begin{corollary}
		Let $\calC$ be a close convex set, $x \in \calC$ and $z \in \RR^n$. If $z \in N_\calC(x)$, then $\forall t\ge 0$, $P_\calC(x+tz)=x$.
	\end{corollary}
	
	Applying the above corollary to opposite of the gradient of a local minimum, thus verifying theorem~\ref{theo:first_order_normal_cone}, gives the next theorem.
	
	\begin{theorem}
	Consider the problem~\eqref{eq:math_prog_geom} where the non empty feasible set $\calC$ is closed and convex. Then $x^*$ is a local minimum if and only if for all $t \ge 0$, $P_\calC\left(x^*-t\nabla f(x^*)\right)=x^*$.
	\end{theorem}
	
	There is our desired result that we shall use as a termination criteria, up to a small tolerance. In practice, we will work with the Cauchy arc defined as
	
	\begin{equation}\label{eq:cauchy_arc}
		p(t,x) := P_\calC(x-t\nabla f(x)),
	\end{equation}
	
	that is the path alongside the steepest descent direction. The Cauchy point is the point minimizing the objective function on the Cauchy arc, i.e.:
	
	\begin{equation}
		x^C := x - t^C \nabla f(x) \text{ where } t^C = \argmin_{t \ge 0} f\left(p(t,x)\right).
	\end{equation}
	
	If an algorithm uses the Cauchy point as the new iterate, then the algorithm would converge. Moreover, if each iterate reduces the objective function as much as a fraction of the Cauchy point, the algorithm would also converge. This pretty much sets the foundation, and intuition, for the theory of convergence for algorithms producing inexact solutions of the sub problems, especially for trust regions methods~\cite{conn-etal:2000}. In a less formal way, it translates into a theoretical notion the practical aspect that computers are by nature inexact, because of floating-point arithmetic, and thus we are more interested into proving that "not exact but good enough" solutions will do the job. An algorithm that does not converge if solutions are not exact might be nice on paper but will not be very useful in practice.
	
	\section{The conjugate gradient method}
	
	In this section, we describe a very popular iterative method in optimization, the conjugate gradient (CG) method. We will try to give a comprehensive review and intuitive view of the method and its main characteristics. 
	
	\subsection{The main algorithm}
	
	Let $H \in \calS_n(\RR)$ such that $H \succ 0$, $c \in \RR^n$ and consider the quadratic program (QP)
	
	\begin{equation}\label{eq:unconstr_qp}
		\min_x \ q(x) = \dfrac{1}{2} \scal{x}{Hx} - \scal{c}{x}.
	\end{equation}
	
	This formulation will often occur in our case when we will formulate sub-problems from a quadratic model of the objective function.
	
	
	By positive-definiteness of $H$, program~\eqref{eq:unconstr_qp} is convex and thus has a unique minimizer $x^*$ verifying $\nabla q(x^*)=0$, i.e.
	\begin{equation}\label{eq:qp_linear_syst}
		Hx^* = c.
	\end{equation}
	Solving a convex QP is then equivalent to solving a symmetric linear system. The question is now how to solve it? A first approach could be to factorize $H$ to transform system~\eqref{eq:qp_linear_syst} into linear systems easier to solve, by using the Cholesky factorization or the QR decomposition for instance. See~\cite{golubvanloan:2013} for details on how to compute them. Using such direct methods can  still be prohibitive in computations and storage\footnote{Expand this paragraph to tell how exact methods can have issues}. The conjugate gradient method has the advantage to be iterative, to only require matrix-vector products and to converge in, at most, $n$ iterations (modulo zero precision arithmetic). Before describing the principle of the method, we will introduce the notion of conjugate vectors w.r.t. a symmetric matrix.
	
	\begin{definition}\label{def:conjugate_vectors}
		Let $H \succ 0$. Two vectors $u$ and $v$ are said to be $H$-conjugate if $\scal{u}{Hv}=0$.
	\end{definition} 
	
	Visually, two $H$-conjugate vectors are orthogonal in the space obtained obtained after applying $H$ to $\RR^n$. This definition naturally extends to a set of vectors $(p_i)_{i\in I}$ who are said to be $H$-conjugate if $\scal{p_i}{Hp_j}=0$ for all $(i,j) \in I^2$ such that $i\neq j$. If the matrix $H$ is implicit, we will just say "conjugate". 
	
	An interesting fact is that $k\le n$ conjugate vectors are also linearly independent. Thus, a set of $n$ conjugate vectors form a basis of $\RR^n$. Let's say that we initiate our iterative process from a given $x_0$ from which we seek to compute our solution $x^*$. If $(p_0,\ldots,p_{n-1})$ is a set of conjugate vectors, then there are scalars $(\alpha_0,\ldots,\alpha_{n-1})$ such that
	\[ x^*-x_0 = \sum_{i=0}^{n-1} \alpha_i p_i.\]
	
	Using the conjugacy property, one can find that the scalars are actually determined by the relation
	
	\[ \alpha_k = \dfrac{\scal{p_k}{H(x^*-x_0)}}{\scal{p_k}{Hp_k}}.\]
	
	The problem is that this relation depends on the solution that we are actually looking for and do not have access to\footnote{if we did, why solving the problem?}. However, this issue can be solved by introducing, for $k\ge 1$, the intermediates iterates 
	
	\[ x_k = x_0 + \sum_{i=0}^{k-1} \alpha_i p_i.\]
	
	The scalars can now be expressed
	
	\begin{equation}\label{eq:cg_steplength_1st_version}
		\alpha_k = -\dfrac{\scal{p_k}{r_k}}{\scal{p_k}{Hp_k}},
	\end{equation}
	
	where $r_k := Hx_k - c$ is the $k$-th residual. By the iterative relation $x_{k} = x_{k-1}+\alpha_{k-1}p_{k-1}$, one also has $r_{k}=r_{k-1} + \alpha_{k-1}Hp_{k-1}$.
	
	The latter relation can also be derived from performing a linesearch along the direction $p_k$ by computing $\alpha_k = \argmin_\alpha q(x_k + \alpha p_k)$. Since $q$ is quadratic, there is an explicit formula that also gives the relation~\eqref{eq:cg_steplength_1st_version}. Yet, we prefer to view this process as an iterative decomposition of the vector $x^*-x_0$ onto a basis of conjugate vectors. For instance, it is trivial that this process converges in $n$ iterations because each iteration consists in computing a coefficient of the decomposition. 
	
	In any case, the coefficients can now be computed recursively, provided we have access to conjugate directions, which is not a trivial question. If one knows its linear algebra classics, since $H$ is symmetric, by the spectral theorem, there is an orthonormal basis of eigen vectors into which $H$ is diagonal. With such vectors, the conjugacy property is clearly satisfied so this basis could do the job. The problem is that diagonalizing a matrix is a very expensive process on its own that could even exceed the cost of a relevant factorization of $H$. 
	
	Also note that having a base diagonalizing $H$ is stronger than what we actually need. Indeed, using matrix notations, let $P$ be the matrix whose columns are simply conjugate vectors $p_1,\ldots,p_n$ and $G \in O(n)$ such that $H=GDG^T$ for some diagonal matrix $D$. Then, because $G^TG=I_n$, one has $G^THG=D$, whereas definition~\ref{def:conjugate_vectors} simply implies that $P^THP$ is diagonal. The latter is weaker, because there is absolutely no reasons for $P$ to be in $O(n)$. Of course, $P$ is invertible but its inverse is not necessarily its transpose. 
	
	Another method would be to apply the Gram-Schmidt orthonormalization process to a set of given vectors. It would work but would still require storing all the vectors, which is less suited for large-scale applications. 
	
	Fortunately, there is a way to produce each direction iteratively in such a way that only accessible information about the problem and the previous basis vector are required to compute a new basis vector. The initial information will consist of the residuals $r_k$. Let $x_0$ be our starting point, $r_0=Hx_0-c$. For $k\ge 1$, every new basis vector is defined by
	
	\[p_k = -r_k + \beta_{k-1} p_{k-1}\]
	
	where $\beta_{k-1}$ is a conjugacy parameter whose expression can be derived from imposing $\scal{p_{k-1}}{Hp_k}=0$ and injecting the above expression:
	
	\[ \beta_{k-1} = \dfrac{\scal{p_{k-1}}{Hr_k}}{\scal{p_{k-1}}{Hp_{k-1}}}.\]
	
	Combined with the recursion previously described, this process respects our conditions to only require accessible information about the previous iteration and ensures the following properties:
	\begin{itemize}
		\item $\forall i<j,\ \scal{r_i}{r_j}=0$
		\item $\forall i\neq j,\ \scal{p_i}{Hp_j}=0$ ($H$-conjugacy).
		\item $\forall k,\ span(r_0,\ldots,r_k) = span(p_0,\ldots,p_k) = span(r_0,Hr_0,\ldots,H^{k-1}r_0)$
	\end{itemize}
	
	The space $span(r_0,Hr_0,\ldots,H^{k-1}r_0)$ is the Krylov subspace of degree $k$, generally written $\calK(H,r_0,k)$, is a the foundation of a category of iterative methods called Krylov methods, of which the CG method is a special instance. 
	
	Before describing the an algorithm, some relations can be modified such that the computations are even more efficient. First, with the conjugacy recursion, one has $\scal{p_k}{r_k} = - \|r_k\|^2$, leading to 
	
	\[\alpha_k = \dfrac{\|r_k\|^2}{\scal{p_k}{Hp_k}}.\]
	
	Similarly, the conjugacy parameter is also equal to 
	
	\[\beta_k = \dfrac{\|r_{k+1}\|^2}{\|r_k\|^2}.\]
	
	The CG method is stated in algorithm~\ref{algo:cg_method}.
	
	\begin{algorithm}
		\caption{The conjugate gradient method}\label{algo:cg_method}
		\begin{algorithmic}
			\Require Starting point $x_0$
			\State $r_0 \gets Hx_0-c$ and $p_0 \gets -r_0$
			\For{$k=0,\ldots,n-2$ \textbf{or} until convergence}
				\State $\alpha_k \gets \frac{\|r_k\|^2}{\|p_k\|^2_H}$
				\State $x_{k+1} \gets x_k+\alpha_kp_k$
				\State $r_{k+1} \gets r_k + \alpha_kHp_k$
				\State $\beta_k =  \frac{\|r_{k+1}\|^2}{\|r_k\|^2}$
				\State $p_{k+1} \gets -r_{k+1}+\beta_kp_k$
			\EndFor{}
			\Return $x_k$
		\end{algorithmic}
	\end{algorithm} 
	
	 One can observe that each iteration only requires one matrix-vector product and inner products, making this method very efficient for large scale applications. To expand an earlier remark, the CG method belongs to the class of Krylov methods who consist in an iterative process for which, starting from $x_0$, each iterate is a minimizer of the objective function over a subspace of the form $\calK(H,r_0,k)$, also called a Krylov subspace. We have only seen an application for square symmetric systems but the theory extend to rectangular and indefinite systems.\footnote{TODO: add a reference to some Krylov methods}
	 
	 As we already said, the CG method converges in at most $n$ iterations, but the efficiency of the method is highly related to the eigenvalues distributions of the matrix.
	 
	 \begin{proposition}
	 	Let $H\in \calS_n^{++}(\RR)$ such that $H$ has $r<n$ distinct eigenvalues, then algorithm~\ref{algo:cg_method} applied to problem~\eqref{eq:unconstr_qp} converges in at most $r$ iterations. 
	 \end{proposition}
	 
	 In a similar fashion, if matrix $H$ has clustered eigenvalues, then the CG method will give a very good solution in a few iterations (as much as they are clusters). 
	 
	 A more numerical result~\citep[][Theorem 5.1.7]{conn-etal:2000} states that, when applying algorithm~\ref{algo:cg_method}, the norm decrease of the error $\epsilon_k := x_k-x_*$ satisfies the inequality
	 \begin{equation}\label{eq:cg_error_decrease_inequality}
	 	\dfrac{\|\epsilon_k\|}{\|\epsilon_0\|_H} \le 2 \left(\dfrac{\sqrt{\kappa(H)}-1}{\sqrt{\kappa(H)}+1}\right).
	 \end{equation}
	 
	 Then, if we could solve another linear system equivalent to~\eqref{eq:qp_linear_syst}  where the matrix has clustered eigenvalues, and thus a condition number closer to $1$, the efficiency of the CG method would be improved. This is the principle of preconditioning, that we will now discuss about.
	 
	 \subsection{Preconditioning}
	 
	 The idea of preconditioning is to apply the CG method to a modified version of system~\eqref{eq:qp_linear_syst} after applying a change of variables of the form $x=R^{-1}\bar{x}$, with $R \in GL_n(\RR)$ is a \textit{preconditioner}. Rewriting the system w.r.t. $\bar{x}$ gives 
	 \begin{equation}\label{eq:cg_preconditioned_system}
	 	\bar{H}\bar{x} = \bar{c},
	 \end{equation}
	 where $\bar{H}:= R^{-T}HR^{-1}$ and $\bar{c}:= R^{-T}c$. This implies that the preconditioner should be chosen such that the transformed Hessian $\bar{H}$ has a better eigenvalue distribution than $H$.
	 
	 One could directly apply algorithm~\ref{algo:cg_method} to system~\eqref{eq:cg_preconditioned_system}, to get the solution $\bar{x}^*$ and then back transform to obtain the solution of the original problem $x^*=R^{-1}\bar{x}^*$. On paper this works but it has the disadvantage to require an explicit computation of $\bar{H}$, which we do not want in practice. Forming this matrix matrix could, for instance, ruin the sparsity pattern of the original matrix $H$. We would prefer to formulate an method where iterates are manipulated implicitly by using their expressions w.r.t. the original variables. 
	 
	 Instead of the matrix $R$, we shall use the matrix $M=R^TR$ that naturally appears when applying CG to system~\eqref{eq:cg_preconditioned_system}. In general, this is the matrix we refer to as the preconditioner, by implicitly imposing it has the suitable format and properties. The Preconditioned Conjugate Gradient (PCG) method is outlined in algorithm~\ref{algo:pcg_method}.
	 
	 \begin{algorithm}
	 	\caption{The preconditioned conjugate gradient method}\label{algo:pcg_method}
	 	\begin{algorithmic}
	 		\Require Starting point $x_0$
	 		\State $r_0 \gets Hx_0-c$, $v_0\gets M^{-1}r_0$ and $p_0 \gets -v_0$
	 		\For{$k=0,\ldots,n-2$ \textbf{or} until convergence}
	 		\State $\alpha_k \gets \frac{\scal{r_k}{v_k}}{\scal{p_k}{Hp_k}}$
	 		\State $x_{k+1} \gets x_k+\alpha_kp_k$
	 		\State $r_{k+1} \gets r_k + \alpha_kHp_k$
	 		\State $v_{k+1}\gets M^{-1}r_{k+1}$
	 		\State $\beta_k =  \frac{\scal{r_{k+1}}{v_{k+1}}}{\scal{r_k}{v_k}}$
	 		\State $p_{k+1} \gets -v_{k+1}+\beta_kp_k$
	 		\EndFor{}
	 		\Return $x_k$
	 	\end{algorithmic}
	 \end{algorithm} 
	 
	 Compared to the vanilla CG, the PCG method comes with the extra cost of solving, at each iteration, a linear system of the form $Mv_k=r_k$. The "ease" with which this system can be solved should therefore be taken into account when choosing a preconditioner. The art of finding a good preconditioner lies in this compromise between the gain in speed of convergence and the extra cost of solving a linear system.
	 
	 Since matrices $M^{-1}H$ and $R^{-T}HR^{-1}$ have the same spectrum, the efficiency of the method depends on how $M$ clusters the eigenvalues of $H$, as it is shown by the following convergence results.
	 
	 \begin{corollary}
	 	\label{corollary:pcg_convergence}
	 	The error $\epsilon_k$ of the iterates generated by algorithm~\ref{algo:pcg_method} satisfy the inequality
	 	\begin{equation}\label{eq:pcg_error_decrease_inequality}
	 		\dfrac{\|\epsilon_k\|_H}{\|\epsilon_0\|_H} \le 2 \left(\dfrac{\sqrt{\kappa(M^{-1}H)}-1}{\sqrt{\kappa(M^{-1}H)}+1}\right).
	 	\end{equation}
	 	Furthermore, if $M^{-1}$ has $r$ distinct eigenvalues, then algorithm~\ref{algo:pcg_method} applied to problem~\eqref{eq:unconstr_qp} converges in at most $r$ iterations.
	 \end{corollary}
	 
	 Multiple choices of preconditioners with good practical performance can be done, although there is no preconditioner that performs well on any problem. As always in optimization, it is important to exploit the structure of a problem to make more efficient algorithms, and preconditioners make no exception. However, the most popular and versatile choice seems to be the Incomplete Cholesky Factorization~\cite{golubvanloan:2013}. Remind that the Cholesky factorization consists in forming $H=L^TL$ for $H \succ 0$, with $L$ lower triangular. When $H\succeq 0$, or even indefinite, the incomplete version consists into the decomposition
	 \[A = L^TL+R\]
	 where $L$ is lower triangular and $R$ is a matrix where the coefficients of $H$ that mess up the computation of the pivots of the Cholesky factorization are thrown into, broadly speaking. In~\cite{linmore:1999b}, proposed an algorithm for a sparse version of this factorization. They use it in the TRON solver~\cite{linmore:1999a} and it gives stellar performances.
	 
	 We will likely use it in our algorithms so stay tuned.
	 
	\subsection{The projected conjugate gradient}
	 
	 We continue our exploration of the CG method and now tackle the linearly constrained case:
	 \begin{equation}\label{eq:qp_linear_constraints}
	 	\begin{aligned}
	 		\min_x & \quad \dfrac{1}{2} \scal{x}{Hx} - \scal{c}{x} \\
	 	\text{s.t.}	& \quad Ax=b,
	 	\end{aligned}
	 \end{equation}
	 
	 where $A \in \calM_{m,n}(\RR)$, $m\le n$. We still have $H\succ 0$ for the convexity and we assume $A$ is of rank $m$ such that the system $Ax=b$ has, at least, one solution for any $b\in\RR^m$. 
	 
	 The idea of the projected CG is to reformulate problem~\eqref{eq:qp_linear_constraints} as an unconstrained problem whose solution will automatically satisfy the constraints. For simplicity and to emphasize on the important ideas of method, we will first consider the case $b=0_{\RR^m}$. Our solution thus lies in the null space of $A$. Remind that $A$ has rank $m$, thus, there exists a matrix $N\in \calM_{n,n-m}(\RR)$ whose columns span the null space of $A$, i.e. $AN=0$. Then, for any $x$ such that $Ax=0$, there is a unique $x^N \in \RR^{n-m}$ such that $x = Nx^N$. If we consider points of this form, the constraints $Ax=0$ are automatically satisfied. Therefore, solving~\eqref{eq:qp_linear_constraints} is equivalent to first solving the unconstrained QP
	  \begin{equation}\label{eq:qp_null_space}
	 		\min_{x^N} q^N(x^N) = \dfrac{1}{2} \scal{x^N}{H^Nx^N} - \scal{c^N}{x^N}
	 \end{equation}
	 where $q^N(x^N) := q(Nx^N)$ (giving $H^N:=N^THN$ and $c^N:=N^Tc$), and then multiply by $N$ to get the desired solution. This would make sense only if $H^N \succ 0$, which we will assume.
	 
	 Projected CG consists into applying the CG method, or the preconditioned version, to problem~\eqref{eq:qp_null_space}.	As for the PCG, algorithm, we prefer to express the iterations in terms of the original variable $x$ and implicitly update the variable $x^N$. This procedure is outlined in algorithm~\ref{algo:projected_pcg_method} and makes use of a preconditioner $M^N$ for $H^N$.
	 
	 \begin{algorithm}
	 	\caption{The projected preconditioned conjugate gradient method (preliminary)}\label{algo:preliminary_projected_pcg_method}
	 	\begin{algorithmic}
	 		\Require Starting point $x_0$ verifying $Ax_0=0$
	 		\State $r_0 \gets Hx_0-c$, $v_0\gets N^T(M^N)^{-1}N^Tr_0$ and $p_0 \gets -v_0$
	 		\For{$k=0,\ldots,n-2$ \textbf{or} until convergence}
	 		\State $\alpha_k \gets \frac{\scal{r_k}{v_k}}{\scal{p_k}{Hp_k}}$
	 			\State $x_{k+1} \gets x_k+\alpha_kp_k$
	 			\State $r_{k+1} \gets r_k + \alpha_kHp_k$
	 			\State $v_{k+1}\gets N{(M^N)}^{-1}N^Tr_{k+1}$
	 			\State $\beta_k =  \frac{\scal{r_{k+1}}{v_{k+1}}}{\scal{r_k}{v_k}}$
	 			\State $p_{k+1} \gets -v_{k+1}+\beta_kp_k$
	 			\EndFor{}
	 			\Return $x_k$
	 		\end{algorithmic}
	 	\end{algorithm} 
	 
	In the projected CG, the extra cost arises from the extra computation of $v_{k+1}= N{(M^N)}^{-1}N^Tr_{k}$. Which, on paper, requires to explicitly form the matrix $N$. We can actually bypass this by rewriting the latter system in an augmented form involving only known quantities. Before that, we shall detail the structure of the preconditioner $M^N$. Since the ideal choice would be $N^THN$, this motivates that $M^N$ should approximate $H^N$ or, equivalently, that $M^N$ has the form 
	\[M^N=N^TMN,\]
	where $M$ is a preconditioner approximating $H$. The system previously discussed now becomes
	 \[v_k = N(N^TMN)^{-1}N^Tr_k\]
	 which does not look more appealing at first glance but is actually is because $P := N(N^TMN)^{-1}N^T$ is a projection matrix. Hence, the properties of projection enable one to compute a matrix vector product involving $P$ through equivalent systems of, maybe, higher dimension, but that preserve the structure of the problem and do not require to explicitly form the matrix $P$. For instance, when $M=I$, $P$ is the orthogonal projection onto the null space of $A$ and verifies
	 \[P = I-A^T(AA^T)^{-1}A.\]
	 Therefore, one can compute $v=Pr$ by solving an\textit{augmented system} or \textit{normal equations}\footnote{We highlight in italic because it is the first time we encounter these words}. Those terms and their associated procedures are discussed in subsection~\ref{subsec:cg_feasible_solution}.
	 Choosing one approach or the other will depend on the structure of the problem, the preconditioner, the access to a factorization of the augmented matrix, etc. The important thing to remember from this discussion is that we do not need to explicitly form the null space matrix $N$.
	 
	 For the case $b\neq 0$, the only difference will be the initialization. Any vector $x$ can be composed into the sum of two components
	 \begin{equation}\label{eq:range_null_space_decomp}
	 	x = x^R + Nx^N,
	 \end{equation}
	 where $N$ is the null space matrix of $A$ as before and $x^R$ satisfies $Ax^R=b$. It is the direct sum structure (special solution) + (element of null space). 
	 
	 Such an $x^R$ can be also be computed by normal equations or an augmented system (see subsection~\ref{subsec:cg_feasible_solution}). With fixed $x^R$, one can substitute~\eqref{eq:range_null_space_decomp} into~\eqref{eq:qp_linear_constraints}. Any point of the form~\eqref{eq:range_null_space_decomp} satisfies the constraints, thus one can remove them and retrieve, as earlier, an unconstrained QP expressed in terms of $x^N$ also involving the reduced Hessian $N^THN$. Only the right hand side $c^N$ will differ.
	 
	 The discussion and strategies about preconditioning are not affected. The only difference with algorithm~\eqref{algo:preliminary_projected_pcg_method} is that we require the initial point to satisfy $Ax=b$. To lighten the notations, we will use the projector preconditioner $P= N(N^TMN)^{-1}N^T$ where $M$ is a preconditioner for $H$. The full procedure of the Projected CG method is outlined in algorithm~\ref{algo:projected_pcg_method} 
	 
	 \begin{algorithm}
	 	\caption{The projected preconditioned conjugate gradient method}\label{algo:projected_pcg_method}
	 	\begin{algorithmic}
	 		\Require Starting point $x_0$ verifying $Ax_0=b$
	 		\State $r_0 \gets Hx_0-c$, $v_0\gets Pr_0$ and $p_0 \gets -v_0$
	 		\For{$k=0,\ldots,n-2$ \textbf{or} until convergence}
	 		\State $\alpha_k \gets \frac{\scal{r_k}{v_k}}{\scal{p_k}{Hp_k}}$
	 		\State $x_{k+1} \gets x_k+\alpha_kp_k$
	 		\State $r_{k+1} \gets r_k + \alpha_kHp_k$
	 		\State $v_{k+1}\gets Pr_{k+1}$
	 		\State $\beta_k =  \frac{\scal{r_{k+1}}{v_{k+1}}}{\scal{r_k}{v_k}}$
	 		\State $p_{k+1} \gets -v_{k+1}+\beta_kp_k$
	 		\EndFor{}
	 		\Return $x_k$
	 	\end{algorithmic}
	 \end{algorithm} 
	 
	 \subsection{Computing a feasible solution and projections}\label{subsec:cg_feasible_solution}
	 
	 As mentioned in the previous section, the projected version of the CG (algorithm~\ref{algo:projected_pcg_method}) requires that the starting point satisfies the linear equality constraints $Ax=b$. We assume $b\neq0$ and voluntarily omit the case where $b=0$ because $x=0$ always works. The goal of this subsection is to explain what the terms \textit{normal equations} and \textit{augmented system} refer to. To draw the big picture, they are different formulations of linear systems that both lead to a feasible solution. We shall see that they also both derive from the same KKT conditions.
	 
	 
	 The term \textit{normal equations} might sound familiar because it refers to the system $A^TAx=A^Tb$ which is nothing but the first order optimality conditions of the linear least-squares problem
	 \begin{equation}\label{eq:linear_least_squares}
	 	\min_x\ \dfrac{1}{2} \left\Vert Ax-b\right\Vert^2,
	 \end{equation} 
	 for the case of $m\le n$. This corresponds to finding the point on the range of $A$ that is the closest to $b$. Our case is different because we have $m < n$ so $A$ is surjective. This means that the system $Ax=b$ has at least one solution, if not several. How to choose? Well, we could for the least norm solution, i.e. the solution of the QP
	 \begin{equation}\label{eq:least_norm_pb}
	 	\begin{aligned}
	 		\min_x & \quad \dfrac{1}{2}\|x\|^2 \\
	 		\text{s.t.} & \quad Ax=b.
	 	\end{aligned}
	 \end{equation}
	 
	 We will not make the joke of saying one can solve this problem by applying algorithm~\ref{algo:projected_pcg_method} but shall derive results from the first-order optimality conditions. The Lagrangian of problem~\eqref{eq:least_norm_pb} is given by
	 \[\ell(x,y) = \dfrac{1}{2}\|x\|^2 + \scal{y}{Ax-b},\]
	 By the full rank assumption, LICQ holds and thus there is $(x,y)$ satisfying the KKT conditions
	 \begin{align*}
	 	x + A^Ty &= 0 \\
	 	Ax &= b,
	 \end{align*} 
	 
	 Which can be written as the system of unknowns $(x,y)$
	 \begin{equation}\label{eq:augmented_system}
	 	\begin{pmatrix}
	 	I & A^T \\
	 	A & 0
	 \end{pmatrix} \begin{pmatrix}
	 	x \\ y 
	 \end{pmatrix} = \begin{pmatrix}
	 	0 \\ b
	 \end{pmatrix}.
	 \end{equation}
	 Solving the latter is the \textit{augmented system} approach. In this system, the variable $y$ plays the role of an auxiliary variable, since we are just interested in $x$. Since the system matrix, also called the augmented matrix, is indefinite, methods to solve~\eqref{eq:augmented_system} require a specific decomposition~\cite{golubvanloan:2013}. 
	 
	 Instead of directly solving~\eqref{eq:augmented_system}, we can also rearrange the equations by substituting the first equation into the second to get
	 \[y=-(AA^T)^{-1}b,\]
	 and then 
	 \begin{equation}\label{eq:normal_eq_system}
	 	x = A^T(AA^T)^{-1}b.
	 \end{equation}
	 
	 Note that the first system is well defined because $A$ full rank implies that $AA^T$ is invertible. One also has $A\succ 0$. This is the \textit{normal equations} approach. Direct methods to solve~\eqref{eq:normal_eq_system} can, for instance, make use of the Cholesky decomposition of $AA^T$.
	 
	 Those approaches can be naturally extended to the preconditioned case. 
	 
	 is equivalent to solving normal equations or the augmented system
	 \[\begin{pmatrix}
	 	I & A^T \\
	 	A & 0
	 \end{pmatrix} \begin{pmatrix}
	 	v \\ w 
	 \end{pmatrix} = \begin{pmatrix}
	 	r \\ 0
	 \end{pmatrix},\]
	 
	 with the addition of an auxiliary variable $w$. A different choice of $H$ would lead to the similar system
	 \begin{equation}\label{eq:augmented_system_preconditionned}
	 	\begin{pmatrix}
	 		H & A^T \\
	 		A & 0
	 	\end{pmatrix} \begin{pmatrix}
	 		v \\ w 
	 	\end{pmatrix} = \begin{pmatrix}
	 		r \\ 0
	 	\end{pmatrix}.
	 \end{equation}
	 
	\section{Generalities on least squares}
	 
	 Rewriting the objective function of problem~\eqref{eq:model_cnls} as $f\colon x \mapsto  \frac{1}{2} \|r(x)\|^2$, one has:
	 
	 \begin{subequations}
	 		\begin{align}
	 		\nabla f(x) &= J(x)^Tr(x)\label{subeq:ls_grad} \\
	 		\nabla^2 f(x) &= J(x)^TJ(x) + S(x) , \label{subeq:ls_hessian}
	 		\end{align}
	 \end{subequations}
	 where $J(x) = \left[\dfrac{\partial r_i}{\partial x_j}\right]_{(i,j)}$ is the Jacobian matrix of the residuals and the second component of the Hessian $S(x) = \sum_{i=1}^{d} r_i(x) \nabla^2r_i(x) $. The latter is expensive in both computational time and storage, since it requires $d$ computations of $n\times n$ matrices. Hence, this component of the Hessian is the one to be approximated. 
	 
	 The Jacobian matrix of constraints function $h$ is noted $A$.
	 
	 
	 
	 We now address is a quick review of the three most popular classes of approximations. For a comprehensive review of these methods, we refer the reader to the chapter 10 of \cite{dennisschnabel:1996}.
	 
	 \subsection{Gauss--Newton method}
	 
	 Originally used by Gauss the prince himself, \textbf{Gauss--Newton} (GN) approximation sets $S(x)$ to the zero matrix. It is the cheapest to compute, since the Jacobian is necessary to evaluate the gradient and works well in practice for zero residuals problems~\cite{dennisschnabel:1996}. When solving problem~\eqref{eq:model_cnls} using an iterative method, this approximation amounts to linearizing the residuals function within the norm. Indeed, approximating $\nabla f^2(x)$ by $J(x)^TJ(x)$ in a quadratic model $\calQ$ of $f$ around $x$ gives
	 
	 \begin{equation}\label{eq:gn_model}
	 	\calQ^{GN} (p) = \dfrac{1}{2}p^T\nabla f^2(x)p + \nabla f(x)^Tp =  \dfrac{1}{2} \left\Vert J(x)p+r(x) \right\Vert^2,
	 \end{equation}
	 
	 which corresponds to injecting the linearization $r(x+p) \approx J(x)p+r(x)$ in the squared norm.
	 
	 \subsection{Levenberg--Marquardt}
	 
	 Next, we describe the \textbf{Levengerg--Marquardt} (LM) method, respectively named after the first author to publish it~\cite{levenberg:1944} and the author of its best-known rediscovery~\cite{marquardt:1963}. As noted by Marquardt in his paper, the two authors started from a different line of reasoning but came to the same conclusion. In this method, matrix $S(x)$ is set to a multiple of the identity matrix $\sigma I$ where $\sigma$ is a positive scalar. The latter is called regularization parameter, because setting  $\nabla f^2(x)$ to $J(x)^TJ(x)$ leads to the quadratic model 
	 
	 \begin{equation}\label{eq:lm_model}
	 	\calQ^{LM}(p) =  \dfrac{1}{2} \left\Vert J(x)p+r(x) \right\Vert^2 + \dfrac{\sigma}{2} \|p\|^2.
	 \end{equation}
	 
	 In other words, one can see very schematically that
	 
	\[\text{LM model} = \text{GN model} + \text{regularization term}.\]
	
	In practice, the LM approximation works well on zero and small residuals problems and tends to be more robust than the GN approximation. It is still cheap to compute but only requires updating the regularization parameter throughout the iterative process. This method is often referred as the early stage of the trust region methods~\cite{conn-etal:2000}. For the link with regularization methods, consider the trust region problem
	 
	 \begin{equation*}
	 	\begin{aligned}
	 		\min_x \quad & f(x) \\
	 		\text{s.t.} \quad & \|x\| \le \Delta.
	 	\end{aligned}
	 \end{equation*}
	 
	 By applying KKT conditions and assuming that there is a minimizer $x^*$ lying on the trust region, i.e. $\|x^*\|=\Delta$, then (under strict complementarity), there is a strictly positive scalar (the multiplier) $\lambda$ such that
	 \[\nabla f(x^*) + \dfrac{2\lambda}{\Delta}x^*=0.\]
	 
	 As a consequence, $x^*$ is a critical point oh the unconstrained regularized problem
	 
	 \[\min_x \quad f(x) + \dfrac{\delta}{2} \|x\|^2,\]
	 
	 where $\delta=\frac{2\lambda}{\Delta}$.
	 
	 Some references for LM methods: \cite{bellavia-etal:2018}
	 
	 \subsection{Quasi--Newton}
	 
	 Finally, one can compute an approximation of $\nabla^2f(x)$ in a similar pattern as in \textbf{quasi-Newton} methods \cite[][Chapter 6]{nocedalwright:2006} but targeted on the second order components. It has a higher computational cost than the previous two but is more accurate on large residuals problems. In~\cite{dennisetal:1981}, the authors exploit this approach in a adaptative scheme, where an estimation of the curvature is used to decide whether or not the quasi-Newton approximation is worth to use compared to the Gauss-Newton one.
	 
	 It is important to bear in mind that choosing between a "cheap" approximation and a quasi-Newton type one implies making compromises. Depending on the initialization, the quasi-Newton approximation will take some iterations to be good and the accuracy will not be there when most needed, i.e. at the starting point potentially far from the solution, and will match the Gauss-Newton close to the solution on small residuals problems. In other words, the quasi-Newton is not accurate enough when most needed and very accurate when a way cheaper alternative does the same job.
	 
	 References for quasi--Newton specialized to least-squares: \cite{dennisetal:1981,yabetakahashi}.
	 
	 \section{Augmented Lagrangian reformulation}\label{sec:about_al}
	 
	 In this section, we introduce the framework of Augmented Lagrangian-based algorithms and describe an application in the least-squares setting of problem~\eqref{eq:model_cnls}.
	 
	 \subsection{Generalities}
	 
	 In order to remain general, we temporarily consider the mathematical program~\eqref{eq:math_prog} and shall comeback to our beloved least-squares shortly after. 
	 We introduce the Augmented Lagrangian (AL) function associated to program~\eqref{eq:math_prog}:
	 
	 \begin{equation}
	 	\label{eq:al}
	 	\Phi_A(x,\lambda,\mu) := f(x) + \scal{\lambda}{h(x)} + \dfrac{\mu}{2} \|h(x)\|^2,
	 \end{equation}
	 
	 where $\lambda\in \Real^m$ is the vector of Lagrange multipliers and $\mu > 0$ is the penalty parameter.
	 
	 Function~\eqref{eq:al} is nothing than the Lagrangian~\eqref{eq:lagrangian} with a quadratic penalty term, hence the adjective \textit{Augmented}.Assume we fixed $\lambda$ and $\mu$, the problem of interest is now the bound constrained program
	 
	 \begin{equation}\label{eq:al_nlp}
	 	\begin{aligned}
	 		\min_x \quad & \Phi_A(x,\lambda,\mu) \\
	 		\text{s.t.} \quad & l \le x \le u.
	 	\end{aligned}
	 \end{equation}
	 
	 AL-based algorithms fall into the class of penalty methods that generally enable one to use iterative methods for unconstrained optimization  while steel achieving feasibility. In our case, moving the nonlinear constraints into the objective simplifies the set of constraints since only bounds constraints are left.
	 
	 One of the pros of AL methods is that they come naturally with an update formula for the multipliers. Let \((x_k,\lambda_k)\) be the current primal-dual iterate, then one can choose \(\lambda_{k+1} \) as
	 
	 \begin{equation}\label{eq:al_multipliers_update}
	 	\lambda_{k+1} = \lambda_k + \mu_kh(x_k).
	 \end{equation}
	 
	 This relation is merely derived from \(\nabla_x\Phi_A=0\) by identifying the right hand side of equation~\eqref{eq:al_multipliers_update} as multipliers satisfying the KKT condition \(\nabla_x \ell =0\).
	 
	 The procedure of an AL method is relatively  and relies on solving successively  problem~\eqref{eq:al_nlp} with respect to the primal variable until a first order critical point of the original problem~\eqref{eq:math_prog} is found. At every iteration, the penalty parameter is increased and the multipliers are updated by formula~\eqref{eq:al_multipliers_update}. This general pattern is outlined in algorithm~\ref{algo:basic_al} and the main steps of the outer iteration are given in  algorithm~\ref{algo:outer_basic_al_trm}.
	 
	 	\begin{algorithm}
	 		\caption{Basic AL algorithm for solving~\eqref{eq:math_prog}}\label{algo:basic_al}
	 		\begin{algorithmic}
	 			\Require Starting point $\left(x_0^s,\lambda_0\right)$, parameter $\mu_0$  parameter $\mu_0$ and tolerances $\omega_*,\omega_0,\eta_*,\eta_0$.
	 			\Repeat
	 			 \State Compute an approximate solution $x_{k}$ of~\eqref{eq:al_nlp} starting from \(x_k^s\) with tolerance $\omega_k$.
	 			\If{\(\|h(x_k)\| < \eta_k\)}
	 				\State Update iterate and increase penalty parameter.
	 				\Else
	 					\State Restart minimization of~\eqref{eq:al_nlp} with a higher penalty parameter.
	 			\EndIf
	 			\State Update tolerances
	 			\Until{\(\|h(x_k)\| < \eta_*\) \textbf{and}  \(\|\nabla_x \ell(x_k,\lambda_k)\| < \omega_*\)}
	 			\State Return current approximate solution.
	 		\end{algorithmic}
	 	\end{algorithm}
	 	
	 	\begin{algorithm}
	 	\caption{Outer iteration of basic AL algorithm with trust region}\label{algo:outer_basic_al_trm}
	 		\begin{enumerate}
	 		\item[\textbf{Step 1: Inner Iteration}]
	 		
	 		Starting from $x_0^s$, approximately solve $\min_x \Phi_A(x,y_k,\mu_k)$ by computing $x_k$ such that \(\left\Vert x_k - P(x_k-\nabla_k\Phi_A)\right\Vert \le \omega_k\)
	 		by a trust region process.
	 		
	 		\textbf{If} $\|h(x_k)\| \le \eta_k$, execute \textbf{Step 2}.
	 		
	 		Otherwise, execute \textbf{Step 3}.
	 		
	 		\item[\textbf{Step 2: Iterate Update}]
	 		
	 		Update $y_{k+1}$ by formula~\eqref{eq:al_multipliers_update} and set $x_{k+1}^s \gets x_k $.
	 		
	 		Choose new tolerances \(\omega_{k+1}, \eta_{k+1}\) and new penalty parameter \(\mu_{k+1}\).
	 		
	 		Increment $k$ and go back to \textbf{Step 1}.
	 		
	 		\item[\textbf{Step 3: Adjustment of the Penalty Parameter}] 
	 		
	 		Choose $\mu_{k+1}$ significantly greater than $\mu_k$.
	 		
	 		Leave the iterate unchanged: \(\left(x_{k+1}^s,\lambda_{k+1}\right) \gets \left(x_k^s,\lambda_k\right)\).
	 		
	 		Go back to \textbf{Step 1}.
	 	\end{enumerate}
	 \end{algorithm}
	 
	 \textbf{TODO How to compute the approximate minimizer? Projected conjugate gradient!}
	 \subsection{Application to structured least-squares}
	 
	 We now consider program~\eqref{eq:model_cnls}, for which the AL function is given by
	 
	 \begin{equation}
	 	\label{eq:lsal}
	 	\Phi_A(x,\lambda,\mu) := \dfrac{1}{2}\|r(x)\|^2 + \scal{\lambda}{h(x)} + \dfrac{\mu}{2} \|h(x)\|^2,
	 \end{equation}
	 
	 Contrary to formulation~\eqref{eq:al}, we keep the linear equality constraints as is and penalize the violation of the nonlinear constraints. Although the computation of the projection onto the set $\calX$ shall differ, the framework remains the same, 
	 One has the following expression of the gradient: 
	 \begin{equation}
	 	\label{eq:al_grad}
	 	\nabla_x \Phi_A(x,\lambda,\mu) = J(x)^Tr(x) + A^T\pi(x,\lambda,\mu),
	 \end{equation}
	 with $\pi(x,\lambda,\mu):=\lambda + \mu h(x)$ is the first-order estimates of the Lagrange multipliers. 
	 
	 The Hessian is given by
	 \begin{equation}\label{eq:al_hessian}
	 	\nabla^2_{xx} \Phi_A(x,\lambda,\mu) = J(x)^TJ(x) + \mu A(x)^TA(x) +  S(x) + \sum_{i=1}^d \nabla^2 h_i(x) \pi(x,\lambda,\mu).
	 \end{equation}
	 
	 For fixed $\lambda$ and $\mu$, reformulating problem~\eqref{eq:model_cnls} with function~\eqref{eq:al} gives the linearly constrained problem
	 
	 \begin{equation}\label{eq:model_cnls_al_reformulation} 
	 	\begin{aligned}
	 		\min_{x} \quad& \Phi_A(x,\lambda,\mu)  \\
	 		\text{s.t.}  \quad & x \in \calX 
	 	\end{aligned}	
	 \end{equation}
	 
	 As for any other AL based algorithm, the idea behind our algorithm is to solve by an iterative method problem~\eqref{eq:model_cnls_al_reformulation} until a first order critical point of problem~\eqref{eq:model_cnls} is found. At every iteration, a the new iterate will be computed after (approximately) solving a trust region subproblem formed after a quadratic model of the AL around the current iterate. 
	 
	 \subsection{Subproblem}\label{subsec:subproblem}
	 
	 Given a primal-dual iterate $(x_k,\lambda_k)$ and a penalty parameter $\mu_k$, we fix $\lambda_k$ and $\mu_k$ and consider a quadratic model of $x\mapsto \Phi_A(x,\lambda_k,\mu_k)$ around $x_k$:
	 \begin{equation}\label{eq:al_quadratic _model}
	 	m_k(x) = m_k(x_k) + \dfrac{1}{2}\scal{p}{H_kp} + \scal{g_k}{x-x_k} + \dfrac{1}{2}\scal{x-x_k}{H_k(x-x_k)},
	 \end{equation}
	 where $m_k(x_k):=\Phi_A(x_k,\lambda_k,\mu_k)$, $g_k:=\nabla_x \Phi_A(x_k,\lambda_k,\mu_k)$ and $H_k:=\nabla^2_{xx} \Phi_A(x_k,\lambda_k,\mu_k)$ or a symmetric approximation of it. For now, we define $\calQ_k$ as the Gauss-Newton model, obtained after merely linearizing the residuals and constraints in expression~\eqref{eq:lsal}, which gives
	 
	 \[H_k = J(x_k)^TJ(x_k) + \mu A(x_k)^TA(x_k).\]
	 
	 Model~\eqref{eq:al_quadratic _model} is appropriate to describe the algorithm in terms of the iterate. When discussing about subproblems, we prefer to emphasis on the step and would then use the following model of the primal reduction $\Phi_A(x_k+p,\lambda_k,\mu_k)-\Phi_A(x_k,\lambda_k,\mu_k)$:
	 \begin{equation}\label{eq:al_quadratic_reduction_model}
	 	\calQ_k(s) = \dfrac{1}{2}\scal{s}{H_ks} + \scal{g_k}{s}.
	 \end{equation}
	 
	 One has $\calQ_k(x-x_k)=m_k(x)-m_k(x_k)$.
	 
	 Vector $s$ denotes the unknown of the subproblem whose (approximate) solution $s_k$ is the step and is used to compute the new iterate $x_{k+1}=x_k+s_k$.
	 
	 The constraints $x_k+s\in \calX$ are satisfied as long as the steps satisfy:
	 \begin{itemize}
	 	\item \(Cs=0\) (provided that $Cx_0=b$)
	 	\item$ x_k-l \le s \le u-x_k$
	 \end{itemize}
	 
	In our method, we incorporate a trust region strategy to control and assert the quality of a step. Therefore, we add to each subproblem a constraint of the form $\|s\|_k \le \Delta_k$ for a radius $\Delta_k > 0$ and a norm $\|\cdot\|_k$. Index $k$ in the latter means that the norm might depend on the iteration. This constraint reflect the domain on which we believe that our model well approximates the true function. We will evaluate the success of an iteration by using the ratio
	
	\begin{equation}\label{eq:tr_ratio}
		\rho_k = \dfrac{\Phi_A(x_{k},\lambda_k,\mu_k)-\Phi_A(x_{k+1},\lambda_k,\mu_k)}{m_k(x_k)-m_k(x_{k+1})}
	\end{equation}
	
	The subproblem of an outer iteration is then given by
	
	\begin{equation}\label{eq:qsubpb_general_form} 
		\begin{aligned}
			\min_{s} \quad& \calQ_k(s)  \\
			\text{s.t.}  \quad &  x_k+s \in \calX \\
			& \|s\|_k \le \Delta_k.
		\end{aligned}	
	\end{equation}
	
	A first sketch of the procedure is drawn in algorithm~\ref{algo:sketch_mcwal}.
	 \begin{algorithm}
	 	\caption{Sketch of MCWAL}\label{algo:sketch_mcwal}
	 	\begin{algorithmic}
	 		\Require $x_0\in \calX$, $\lambda_0$, $\mu_0, \tau_0$ and constants $\eta_s$
	 		\While{\textbf{not optimal}\footnote{replace with a numerical criteria corresponding to KKT condition}}
	 		\State Evaluate $H_k$ and $g_k$ 
	 		\State Compute a solution $s_k$ of subproblem~\eqref{eq:qsubpb_general_form} 
	 		\State Compute ratio $\rho_k$~\eqref{eq:tr_ratio}
	 		\If{$\rho_k \ge \eta_s$} \Comment{Good step}
	 		\State $x_{k+1} \gets x_k+s_k$
	 		\State Choose $\Delta_{k+1} > \Delta_k$
	 		\If{$\|h(x_k)\| \le \tau_k$}
	 		\State $y_{k+1} \gets \pi(x_k,y_k,\mu_k)$
	 		\State Choose $\mu_{k+1} > \mu_k$ and $\tau_{k+1} < \tau_k$
	 		\Else
	 		\State $y_{k+1} \gets y_k$
	 		\State Choose $\mu_{k+1} < \mu_k$
	 		\EndIf
	 		\Else \Comment{Bad step}
	 		\State  $x_{k+1} \gets x_k$
	 		\State  $y_{k+1} \gets y_k$
	 		\State  $\mu_{k+1} \gets \mu_k$
	 		\State Choose $\Delta_{k+1} < \Delta_k$
	 		\EndIf
	 		\EndWhile
	 	\end{algorithmic}
	 \end{algorithm}
	 
	 \subsection{The inner iteration}
	 
	 In this subsection, we present how to solve subproblem~\eqref{eq:qsubpb_general_form}. Let's consider the current feasible iterate $x_k$ and it's associate approximate quadratic model $\calQ_k$ as defined in the previous section.
	 We seek to compute the step $s_k$ as an approximate solution of the program
	 
	 \begin{equation*}
	 	\begin{aligned}
	 		\min_{s} \quad& \calQ_k(s)  \\
	 		\text{s.t.}  \quad & x_k+s \in \calX \\ 
	 		& \|s\|_\infty \le \Delta_k,
	 	\end{aligned}	
	 \end{equation*}
	 where we have set $\|\cdot\|_k=\|\cdot\|_\infty:x \mapsto \max_i |x_i|$.
	 
	 Since the infinite-norm trust region constraint is equivalent to imposing $x_i \in [-\Delta_k,\Delta_k]$ for all $i$, we can rewrite this subproblem

	\begin{equation}\label{eq:qsubpb} 
		\begin{aligned}
			\min_{p} \quad& \calQ_k(p)  \\
			\text{s.t.}  \quad &  \scal{c_i}{s} = 0,\quad i=1,\ldots,m \\
			& l^{(k)} \le s \le u^{(k)},
		\end{aligned}	
	\end{equation}
	with $l^{(k)}:= \max(-\Delta_k,l-x_k)$ and $u^{(k)}:= \min(\Delta_k,u-x_k)$ (maximums and minimums are taken component wise).
	
	The optimization process consists into applying the gradient projection method. This consists into applying the conjugate gradient method until a bound is reached and restart with a new active set until a termination criteria is full filled.
	
	Using $x_k$, we first find a starting point $x_k^s$ that achieves a sufficient reduction, such as the Cauchy point. Bound constraints that are active as the latter are fixed and described by the set of index $\calA(x_k^s)$. 
	
	In this context, $\calA(x):=\left\{i\ \vert \ x_i \in \left\{l^{(k)}_i,u^{(k)}_i\right\}\right\}$.
	
	Then, we follow~\cite{linmore:1999a} and build the next iterate $x_{k+1}$ after a sequence of $q+1$ minor iterates $x_{k,1},\ldots,x_{k,q+1}$ such that
	\begin{itemize}
		\item $x_{k}^s=x_{k,1}$
		\item $x_{k+1}=x_{k,q+1}$ 
	\end{itemize}
	
	The starting point will be a Cauchy step in the generalized sense of~\cite[][Chapter 12]{conn-etal:2000}, i.e. a minimizer of the quadratic model along the projected steepest direction. In order to compute it, we define the Cauchy step $s^C_k$ as $s_k(t_k)$, where 
	\[s_k(t)=P_\calX(x_k-tg_k)-x_k \text{ for } t\ge 0.\]
	Following the notations introduced in section~\eqref{sec:nlp_optimality_conditions}, $P_\calX$ denotes the orthogonal projector operator on the feasible set $\calX$. The scalar $t_k$ is chosen such that the Cauchy point $x_k^C:= x_k+s^C_k$ ensures a sufficient reduction of the objective function. We will explain later in that document what we mean by "sufficient".
	
	We aim an improved reduction of the objective function at each minor iterate, i.e. we want the following inequality
	\begin{equation}
		m_k(x_{k,j+1}) \le m_k(x_{k,j}).
	\end{equation}
	to be satisfied for all $1\le j \le q$.
	
	Each minor iterate is defined after the previous one and is decomposed into 
	\[x_{k,j+1}=x_{k,j}+\alpha_{k,j}w_{k,j},\]
	where $w_{k,j}$ is an approximate minimizer of the subproblem
	\begin{equation}\label{eq:minor_subpb}
		\begin{aligned}
			\min_w \quad & m_k(x_{k,j}+w) \\
			\text{s.t.} \quad & Cw=0 \\
			& w_i = 0,\quad i \in \calA(x_{k,j}).
		\end{aligned}
	\end{equation}
	The scalar $\alpha_{k,j}$ is computed by a projected search along direction $w_{k,j}$, i.e. such that $P_\calX(x_{k,j}+\alpha_kw_{k,j})$ provides a sufficient reduction.
	
	Problem~\eqref{eq:minor_subpb} is solved by applying a truncated version of the projected conjugate gradient algorithm~\ref{algo:projected_pcg_method} with $x_{k,j}$ as a starting point. Two cases occur\footnote{A third case could occur: negative curvature. For now, we do not handle it but it should be similar as the normal termination case}, depending on wether or not a direction generated by the CG procedure hit one of the bounds $l^{(k)}$ or $u^{(k)}$ before convergence of the CG algorithm.
	
	If an iteration generates a direction that makes the step hit a bound, we stop the algorithm, return the current point $w_{k,j}$ and perform the projected search to compute the next minor iterate $x_{k,j+1}$. We then add the newly active bounds constraints to $\calA(x_{k,j+1}) \supset \calA(x_{k,j})$ and go back to solving~\eqref{eq:minor_subpb}.
	
	If the CG algorithm stops with a convergence criteria, this means we found a local minimizer and then, we only compute the projected search to compute $x_{k,j+1}$and stop the procedure. Formally, we set the remaining minor iterates to $x_{k,j+1}$.
	
	After computing all the minor iterates, the step is defined as $s_k = x_{k,q+1}-x_k$.
	
	\clearpage
	
	\bibliographystyle{plainnat}
	\bibliography{refs}
\end{document}