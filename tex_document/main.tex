%%% ArXiv template from https://www.overleaf.com/latex/templates/an-arxiv-template/gbzmznbxvwpr

\documentclass[10pt]{article}
\usepackage{graphicx}
\baselineskip=16pt

\usepackage{indentfirst,csquotes}

\topmargin= .5cm
\textheight= 20cm
\textwidth= 32cc
\baselineskip=16pt

\evensidemargin= .9cm
\oddsidemargin= .9cm

\usepackage{amssymb,amsthm,amsmath}
\usepackage{xcolor,paralist,hyperref,fancyhdr,etoolbox}


\newtheorem{theorem}{Theorem}[]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}


\hypersetup{colorlinks=true, linkcolor=black, filecolor=black, urlcolor=black }
\def\proof{\noindent {\it Proof. $\, $}}
\def\endproof{\hfill $\Box$ \vskip 5 pt }









%%% PERSONAL ADD-ONS
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\allowdisplaybreaks
\numberwithin{equation}{section}
%\usepackage{natbib}
\include{macros}

\begin{document}
	
	
	\title{Ongoing work on MCWAL} %%%%%%%%%%%%
	\author{Pierre Borie}
%	\date{\today}
	
	
	
	\maketitle
	
	\begin{abstract}
		\noindent This informal document  reflects the ongoing work and thinking on a algorithm for constrained nonlinear least squares. The current algorithm (rapper) name is MCWAL for Moindres Carr\'es With Augmented Lagrangian.
	\end{abstract} %%%%%%%%% 
	

	\section{Introduction}\label{sec:intro}
	
	We consider least squares problems subject to both nonlinear and linear constraints of the form
	\begin{equation}
		\label{eq:model_cnls}
		\begin{aligned}
			\min_{x\in \RR^n} \quad & \dfrac{1}{2} \|r(x)\|^2 \\
			\text{s.t.} \quad & h(x) = 0 \\
			& \scal{c_i}{x} = b_i,\quad i=1,\ldots,m \\
			& \ell \le x \le u,
		\end{aligned}
	\end{equation}
	where $r\colon \RR^n \to \RR^d$  and $h\colon \RR^n \to \RR^t$ are assumed to be nonlinear, potentially non convex, continuously differentiable functions, $\scal{\cdot}{\cdot}$ is the canonical inner product and $\|\cdot\| $  its induced euclidean norm, $c_i$ are $m$  independent vectors of $\Real^n$, $( m \le n)$, $b=(b_1,\ldots,b_m)^T \in \RR^m$ and $\ell$ and $u$ are vectors in $\RR^n$. Without loss of generality, some components of the latter two vectors can be set to $\pm \infty$ for unbounded parameters. In the context of least squares problems, components $r_i$ of the function $r$ are often denoted as the residuals.
	
	We will also refer to the linear constraints using the following set notation 
	
	\begin{equation}
		\label{eq:linear_constraints}
		\calX = \left\{ x \in \RR^n \ | \ Cx=b,\ l \le x \le u\right\},
	\end{equation}
	where $C$ is the matrix whose columns are the vectors $c_i$. By linear independence of those vectors, $C$ is a full rank matrix. The set $\calX$ is thus convex.
	
	\subsection{Notations and reminders of nonlinear programming}
	
	The Jacobian matrix of constraints function $h$ is noted $A$.
	
	When considering iterative methods for solving problem~\eqref{eq:model_cnls}, $k$ will, if not mentioned otherwise, refer to the iteration number. In order to simplify notations, quantities relative to a given iteration will be noted with the iteration number as an index, such as $x_k$ for the iterate, $r_k$ for $r(x_k)$ ,$J_k$ for $J(x_k)$ etc.
	
	Symbol $:=$ shall be used to state the definition of a numerical object (function, vector etc.)
	
	In this section we consider the general mathematical program\footnote{Shall I write the KKT conditions w.r.t this formulation and then consider multipliers for the bounds?}
	\begin{equation}
		\label{eq:math_prog}
		\begin{aligned}
			\min_x \quad & f(x) \\
			\text{s.t.} \quad & h(x)=0 \\
			& l \le x \le u
		\end{aligned}
	\end{equation}
	with the same assumptions on of differentiability of functions $f$ and \(h\). We introduce the Lagrangian associated to problem~\eqref{eq:math_prog}:
	\begin{equation}
		\label{eq:lagrangian}
		\ell(x,\lambda) = f(x) + \scal{\lambda}{h(x)},
	\end{equation}
	where $\lambda$ is the vector of Lagrange multipliers. Algorithms discussed aim to find local minimum of problem~\eqref{eq:math_prog}, i.e. feasible and of minimal value in s neighborhood. Under suitable assumptions, one can establish necessary and even sufficient conditions for local optimality. One of the most important assumptions belongs to the familty of constraints qualifications. The following is the one we will employ.
	
	\begin{definition}\label{def:licq}
		\textbf{(LICQ)}
		The LICQ holds at $x^*$ if the gradients of the equality constraints evaluated at $x^*$ are linearly independent. In other terms, the matrix $A(x)$ is full rank.
	\end{definition}
	
	Using this and standard differentiability assumptions, one can state first-order necessary optimality conditions, also known as KKT conditions. 
	
	\begin{definition}\label{def:kkt_point}
		\textbf{(KKT conditions)}
		
		A point $x^*$ satisfies the KKT conditions if $x^* $ is feasible and there exists multipliers \(\lambda^*\) such that \[\nabla_x \ell (x^*,\lambda^*)=0.\]
	\end{definition}
	
	
	A point satisfying those conditions if also said to be a KKT point or a first order critical point for problem~\eqref{eq:math_prog}. Depending on the context, we would refer to a KKT point either by just wrting $x^*$ or the couple formed after $x^*$ and its associated Lagrange multiplier $\lambda^*$The necessary conditions follow.
	
	\begin{theorem}\label{theo:fonc}
		\textbf{(First Order Necessary Conditions)\cite[][Theorem 12.1]{nocedalwright:2006}}
		
		Let $x^*$ be a local solution of~\eqref{eq:math_prog} at which the LICQ holds. Then $x^*$ is a KKT point.
	\end{theorem}
	
	Sufficient optimality conditions can be established using second order information.
	
	\begin{definition}\label{def:soc}
		\textbf{(Second Order Conditions)}
		
		A point $(x^*,\lambda^*)$ satisfies the second order conditions if it is a KKT point for ~\eqref{eq:math_prog} at which the LICQ holds and if the matrix $\nabla_{xx}^2\ell(x^*,\lambda^*)$ is positive definite on the null space of the constraints Jacobian, i.e.:
		\[\forall w \text{ verifying } \scal{A(x^*)}{w}=0,\ \scal{w}{\nabla_{xx}^2\ell(x^*,\lambda^*)w} > 0.\]
	\end{definition}
	
	\begin{theorem} \textbf{(Second Order Sufficient Conditions) \cite[][Theorem 12.5]{nocedalwright:2006}}
		If $x^*$ satisfies the second order conditions, then $x^*$ is a local minimum of~\eqref{eq:math_prog}.
	\end{theorem}
	
	\subsection{Generalities on least squares}
	 
	 Rewriting the objective function of problem~\eqref{eq:model_cnls} as $f\colon x \mapsto  \frac{1}{2} \|r(x)\|^2$, one has:
	 
	 \begin{subequations}
	 		\begin{align}
	 		\nabla f(x) &= J(x)^Tr(x)\label{subeq:ls_grad} \\
	 		\nabla^2 f(x) &= J(x)^TJ(x) + S(x) , \label{subeq:ls_hessian}
	 		\end{align}
	 \end{subequations}
	 where $J(x) = \left[\dfrac{\partial r_i}{\partial x_j}\right]_{(i,j)}$ is the Jacobian matrix of the residuals and the second component of the Hessian $S(x) = \sum_{i=1}^{d} r_i(x) \nabla^2r_i(x) $. The latter is expensive in both computational time and storage, since it requires $d$ computations of $n\times n$ matrices. Hence, this component of the Hessian is the one to be approximated. 
	 
	 The Jacobian matrix of constraints function $h$ is noted $A$.
	 
	 When considering iterative methods for solving problem~\eqref{eq:model_cnls}, $k$ will, if not mentioned otherwise, refer to the iteration number. In order to simplify notations, quantities relative to a given iteration will be noted with the iteration number as an index, such as $x_k$ for the iterate, $r_k$ for $r(x_k)$ ,$J_k$ for $J(x_k)$ etc.
	 
	 Symbol $:=$ shall be used to state the definition of a numerical object (function, vector etc.)
	 
	 We now address is a quick review of the three most popular classes of approximations. For a comprehensive review of these reviews, we refer the reader to the chapter 10 of \cite{dennisschnabel:1996}.
	 
	 The \textbf{Gauss--Newton} approximation sets $S(x)$ to the zero matrix. It is the cheapest to compute, since the Jacobian is necessary to evaluate the gradient and works well in practice for zero residuals problems~\cite{dennisschnabel:1996}. When solving problem~\eqref{eq:model_cnls} using an iterative method, this approximation amounts to linearizing the residuals function within the norm.
	 
	 Next, the \textbf{Levengerg--Marquardt} (LM) method~\cite{levenberg:1944,marquardt:1963} sets $S(x)$ to a multiple of the identity matrix $\sigma I$ where $\sigma$ is a regularization parameter. It is still cheap to compute but only requires updating the regularization parameter throughout the iterative process. This method can be seen as the early stage of the trust region methods~\cite{conn-etal:2000}. It works well in practice on zero and small residuals problems. 
	 
	 Finally, one can compute an approximation of $\nabla^2f(x)$ in a similar pattern as in \textbf{quasi-Newton} methods \cite[][Chapter 6]{nocedalwright:2006} but targeted on the second order components. It has a higher computational cost than the previous two but is more accurate on large residuals problems. In~\cite{dennisetal:1981}, the authors exploit this approach in a adaptative scheme, where an estimation of the curvature is used to decide whether or not the quasi-Newton approximation is worth to use compared to the Gauss-Newton one.
	 
	 It is important to bear in mind that choosing between a "cheap" approximation and a quasi-Newton type one implies making compromises. Depending on the initialization, the quasi-Newton approximation will take some iterations to be good and the accuracy will not be there when most needed, i.e. at the starting point potentially far from the solution, and will match the Gauss-Newton close to the solution on small residuals problems. In other words, the quasi-Newton is not accurate enough when most needed and very accurate when a way cheaper alternative does the same job.
	 
	 
	 
	 \section{Augmented Lagrangian reformulation}
	 
	 In this section, we introduce the framework of Augmented Lagrangian-based algorithms and describe an application to our least-squares case.
	 
	 \subsection{Generalities}
	 
	 In order to remain general, we temporarly consider the mathematical program~\eqref{eq:math_prog} and shall comeback to the least-squares shortly after. 
	 We introduce the Augmented Lagrangian (AL) function associated to program~\eqref{eq:math_prog}:
	 
	 \begin{equation}
	 	\label{eq:al}
	 	\Phi_A(x,\lambda,\mu) := f(x) + \scal{\lambda}{h(x)} + \dfrac{\mu}{2} \|h(x)\|^2,
	 \end{equation}
	 
	 where $\lambda\in \Real^m$ is the vector of Lagrange multipliers and $\mu > 0$ is the penalty parameter.
	 
	 Function~\eqref{eq:al} is nothing than the Lagrangian~\eqref{eq:lagrangian} with a quadratic penalty term, hence the adjective \textit{Augmented}. The problem of interest is now
	 
	 \begin{equation}
	 	\label{eq:general:al}
	 \end{equation}
	 
	 AL-based algorithms fall into the class of penalty methods that generally enable one to use iterative methods for unconstrained optimization  while steel achieving feasibility. In our case, moving the nonlinear constraints into the objective simplifies the set of constraints since only bounds constraints are left.
	 
	 
	 \subsection{Application to structured least-squares}
	 
	 We now consider program~\eqref{eq:model_cnls}, for which the AL function is given by
	 
	 \begin{equation}
	 	\label{eq:lsal}
	 	\Phi_A(x,\lambda,\mu) := \dfrac{1}{2}\|r(x)\|^2 + \scal{\lambda}{h(x)} + \dfrac{\mu}{2} \|h(x)\|^2,
	 \end{equation}
	 
	 Contrary to formulation~\eqref{eq:al}, we keep the linear equality constraints as is and penalize the violation of the nonlinear constraints.The framework remains the same, only the computation of the projection onto the set $\calX$ shall differ.
	 One has the following expression of the gradient: 
	 \begin{equation}
	 	\label{eq:al_grad}
	 	\nabla_x \Phi_A(x,\lambda,\mu) = J(x)^Tr(x) + A^T\pi(x,\lambda,\mu),
	 \end{equation}
	 with $\pi(x,\lambda,\mu):=\lambda + \mu h(x)$ is the first-order estimates of the Lagrange multipliers. 
	 
	 The Hessian is given by
	 \begin{equation}\label{eq:al_hessian}
	 	\nabla^2_{xx} \Phi_A(x,\lambda,\mu) = J(x)^TJ(x) + \mu A(x)^TA(x) +  S(x) + \sum_{i=1}^d \nabla^2 h_i(x) \pi(x,\lambda,\mu).
	 \end{equation}
	 
	 For fixed $\lambda$ and $\mu$, reformulating problem~\eqref{eq:model_cnls} with function~\eqref{eq:al} gives the linearly constrained problem
	 
	 \begin{equation}\label{eq:model_cnls_al_reformulation} 
	 	\begin{aligned}
	 		\min_{x} \quad& \Phi_A(x,\lambda,\mu)  \\
	 		\text{s.t.}  \quad & x \in \calX 
	 	\end{aligned}	
	 \end{equation}
	 
	 As for any other AL based algorithm, the idea behind MCWAL is to solve by an iterative method problem~\eqref{eq:model_cnls_al_reformulation} until a first order critical point of problem~\eqref{eq:model_cnls} is found. At every iteration, a the new iterate will be computed after (approximately) solving a trust region subproblem formed after a quadratic model of the AL around the current iterate. 
	 
	 \subsection{Subproblem}
	 
	 Given a primal-dual iterate $(x_k,\lambda_k)$ and a penalty parameter $\mu_k$, we consider a quadratic model of the AL around $x_k$:
	 
	 \begin{equation}\label{eq:quadratic_al}
	 	\calQ_k(p) = \dfrac{1}{2}\scal{p}{H_kp} + \scal{g_k}{p},
	 \end{equation}
	 
	 where $H_k:=\nabla^2_{xx} \Phi_A(x_k,\lambda_k,\mu_k)$ or an approximation of it and $g_k:=\nabla_x \Phi_A(x_k,\lambda_k,\mu_k)$.
	 
	 Vector $p$ denotes the unknown of the subproblem whose (approximate) solution $p_k$ shall be used to compute the new iterate $x_{k+1}=x_k+p_k$.
	 
	 For the linear constraints, we want $x_k+p\in \calX$ which will be provided if:
	 \begin{itemize}
	 	\item \(Cp=0\) (provided that $Cx_0=b$)
	 	\item$ x_k-l \le p \le u-x_k$
	 \end{itemize}
	 The above conditions shall be written $p\in \calX_k$ where $\calX_k:= \left\{p \ | \ Cp=0,\ x_k-l \le p \le u-x_k\right\}$.
	 
	As part of our method, we will also add a trust region constraint of the form $\|p\|_k \le \Delta_k$ for a radius $\Delta_k$ and a norm $\|\cdot\|_k$. Index $k$ in the latter means that the norm might depend on the iteration. A priori, we would use the euclidean norm.
	
	The subproblem of an outer iteration is then given by
	
	\begin{equation}\label{eq:quadratic_subpb} 
		\begin{aligned}
			\min_{p\in \calX_k} \quad& \calQ_k(p)  \\
			\text{s.t.}  \quad &  \|p\|_k \le \Delta_k.
		\end{aligned}	
	\end{equation}
	
	A first sketch of the MCWAL procedure is drawn in algorithm~\ref{algo:sketch_mcwal}.
	 \begin{algorithm}
	 	\caption{Sketch of MCWAL}\label{algo:sketch_mcwal}
	 	\begin{algorithmic}
	 		\Require $x_0\in \calX$, $\lambda_0$, $\mu_0, \tau_0$ and constants $\eta_s$
	 		\While{\textbf{not optimal}\footnote{replace with a numerical criteria corresponding to KKT condition}}
	 		\State Evaluate $H_k$ and $g_k$ 
	 		\State Compute a solution $p_k$ of subproblem~\eqref{eq:quadratic_subpb} 
	 		\State Compute ratio $\rho_k$\footnote{TODO: define its expression}
	 		\If{$\rho_k \ge \eta_s$} \Comment{Good step}
	 		\State $x_{k+1} \gets x_k+p_k$
	 		\State Choose $\Delta_{k+1} > \Delta_k$
	 		\If{$\|h(x_k)\| \le \tau_k$}
	 		\State $y_{k+1} \gets \pi(x_k,y_k,\mu_k)$
	 		\State Choose $\mu_{k+1} > \mu_k$ and $\tau_{k+1} < \tau_k$
	 		\Else
	 		\State $y_{k+1} \gets y_k$
	 		\State Choose $\mu_{k+1} < \mu_k$
	 		\EndIf
	 		\Else \Comment{Bad step}
	 		\State  $x_{k+1} \gets x_k$
	 		\State  $y_{k+1} \gets y_k$
	 		\State  $\mu_{k+1} \gets \mu_k$
	 		\State Choose $\Delta_{k+1} < \Delta_k$
	 		\EndIf
	 		\EndWhile
	 	\end{algorithmic}
	 \end{algorithm}
	
	\clearpage
	
	\bibliographystyle{plainnat}
	\bibliography{refs}
\end{document}