\documentclass[10pt]{article}
\usepackage{graphicx}
\baselineskip=16pt

\usepackage{indentfirst,csquotes}

\topmargin= .5cm
\textheight= 20cm
\textwidth= 32cc
\baselineskip=16pt

\evensidemargin= .9cm
\oddsidemargin= .9cm

\usepackage{amssymb,amsthm,amsmath}
\usepackage{xcolor,paralist,hyperref,fancyhdr,etoolbox}


\newtheorem{theorem}{Theorem}[]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}


\hypersetup{colorlinks=true, linkcolor=black, filecolor=black, urlcolor=black }
\def\proof{\noindent {\it Proof. $\, $}}
\def\endproof{\hfill $\Box$ \vskip 5 pt }









%%% PERSONAL ADD-ONS
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\allowdisplaybreaks
%\numberwithin{equation}{section}
%\usepackage{natbib}
\include{macros}

% For foot notes in author name
\newcommand{\footremember}[2]{%
	\footnote{#2}
	\newcounter{#1}
	\setcounter{#1}{\value{footnote}}%
}
\newcommand{\footrecall}[1]{%
	\footnotemark[\value{#1}]%
} 

\begin{document}
	
	
	\title{Augmented Lagrangian for Least-Squares} %%%%%%%%%%%%
	\author{Pierre Borie\footremember{1}{University of Montreal, Department of Computer Science and Operations Research, Montreal, QC, Canada}}
	\date{}
	
	
	
	\maketitle
	
	\section*{Notations}
	We consider constrained nonlinear least squares problems of the form
	\begin{equation}
		\label{eq:model_cnls}
		\begin{aligned}
			\min_{x\in \RR^n} \quad & \dfrac{1}{2} \|r(x)\|^2 \\
			\text{s.t.} \quad & h(x) = 0 \\
			& \scal{c_i}{x} = b_i\quad i=1,\ldots,m \\
			& l \le x \le u,
		\end{aligned}
	\end{equation}
	where $r\colon \RR^n \to \RR^d$  and $h\colon \RR^n \to \RR^t$ are assumed to be nonlinear, potentially non convex, continuously differentiable functions, $\scal{\cdot}{\cdot}$ is the canonical inner product and $\|\cdot\| $  its induced euclidean norm, $c_i$ are $m$  independent vectors of $\Real^n$, $( m \le n)$, $b=(b_1,\ldots,b_m)^T \in \RR^m$ and $\ell$ and $u$ are vectors in $\RR^n$. Without loss of generality, some components of the latter two vectors can be set to $\pm \infty$ for unbounded parameters. In the context of least squares problems, components $r_i$ of the function $r$ are often denoted as the residuals.
	
	We will also refer to the linear constraints using the following set notation 
	
	\begin{equation}
		\label{eq:linear_constraints}
		\calX = \left\{ x \in \RR^n \ | \ Cx=b,\ l \le x \le u\right\},
	\end{equation}
	where $C$ is the matrix whose columns are the vectors $c_i$. By linear independence of those vectors, $C$ is a full rank matrix.
	
	The method presented in this report is conceived around an Augmented Lagrangian (AL) function where only the violation of the nonlinear constraints is penalized. Its expression is given by	
	\begin{equation}
		\label{eq:lsal}
		\Phi_A(x,\lambda,\mu) := \dfrac{1}{2}\|r(x)\|^2 + \scal{\lambda}{h(x)} + \dfrac{\mu}{2} \|h(x)\|^2,
	\end{equation}
	We keep the linear constraints as is and only penalize the violation of the nonlinear constraints. 
	One has the following expression of the gradient: 
	\begin{equation}
		\label{eq:al_grad}
		\nabla_x \Phi_A(x,\lambda,\mu) = J(x)^Tr(x) + A^T\pi(x,\lambda,\mu),
	\end{equation}
	with $\pi(x,\lambda,\mu):=\lambda + \mu h(x)$ is the first-order estimates of the Lagrange multipliers. 
	
	The Hessian is given by
	\begin{equation}\label{eq:al_hessian}
		\nabla^2_{xx} \Phi_A(x,\lambda,\mu) = J(x)^TJ(x) + \mu A(x)^TA(x) +  \sum_{i=1}^d r_i(x)\nabla^2r_i(x) + \sum_{i=1}^d \nabla^2 h_i(x) \pi(x,\lambda,\mu).
	\end{equation}
	
	For fixed $\lambda$ and $\mu$, replacing the objective function with the AL gives the linearly constrained problem
	
	\begin{equation}\label{eq:model_cnls_al_reformulation} 
		\begin{aligned}
			\min_{x} \quad& \Phi_A(x,\lambda,\mu)  \\
			\text{s.t.}  \quad & x \in \calX.
		\end{aligned}	
	\end{equation}
	
	A local minimum $x^*$ of \eqref{eq:model_cnls_al_reformulation} then satisfies the first-order necessary condition
	\[x^*-P_\calX(x^* -\nabla_x\Phi_A(x^*,\lambda,\mu)=0,\]
	where $P_\calX$ denotes the projection operator onto the set $\calX$.
	\section{The method}
	
	Let's consider a primal-dual iterate $(x_k,\lambda_k)$ such that $x_k \in \calX$ and a penalty parameter $\mu_k$. Each iteration consists into computing an approximating minimizer $x_{k+1}$ of~\eqref{eq:model_cnls_al_reformulation} starting from $x_k$ and such that
	\begin{equation}\label{eq:al_approximate_optimality}
		\left\Vert x_{k+1}-P_\calX(x_{k+1}-\nabla_x\Phi_A(x_{k+1},\lambda_k,\mu_k)\right\Vert \le \omega_k,
	\end{equation}
	
	where $\omega_k$ is a small tolerance. Index $k$ means that we will adapt its value at each iteration (the closer from the solution, the small it will be). The iterate is computed by a trust region method. To do so, we first construct a quadratic model of the AL around $x_k$:
	
	\begin{equation}\label{eq:quadratic_al}
		\calQ_k(p) = \dfrac{1}{2}\scal{p}{H_kp} + \scal{g_k}{p},
	\end{equation}
	
	with $H_k\approx\nabla^2_{xx} \Phi_A(x_k,\lambda_k,\mu_k)$  and $g_k:=\nabla_x \Phi_A(x_k,\lambda_k,\mu_k)$. We compute the step $p_k$ by approximately minimizing the quadratic program
	\begin{equation}\label{eq:quadratic_subpb} 
		\begin{aligned}
			\min_{p} \quad& \calQ_k(p)  \\
			\text{s.t.}  \quad &  x_k+p\in \calX\\
			& \|p\|_\infty \le \Delta_k.
		\end{aligned}	
	\end{equation}
	
	Since we assumed $x_k$ is feasible, it is sufficient that the step satisfies:
	\begin{itemize}
		\item \(Cp=0\)
		\item $ \bar{l} = \max(x_k-l,\Delta_k) \le p \le \min (u-x_k,\Delta_k) = \bar{u}$,
	\end{itemize}
	(the $\max$ and $\min$ are applied component-wise).
	
	If the candidate step $x_k+p_k$ does not satisfy~\eqref{eq:al_approximate_optimality}, we decrease the trust region radius $\Delta_k$ and restart the minimization procedure.
	Once the conditions is verified, we update the iterate by $x_{k+1}=x_k+p_k$ and then check the violation of the nonlinear constraints. 
	
	If the violation is lower than a given tolerance $\eta_k$, (depends on the iteration), than the step is fully accepted. We thus update the Lagrange multipliers by the first-order estimates, slightly increase the penalty parameter $\mu_k$ and decrease the tolerances $\omega_k, \eta_k$. If the constraints violation condition is not satisfied, this means that the next iteration should emphasis on feasibility. We thus keep the Lagrange multipliers and tolerances but increase more significantly the penalty parameter. 
	
	
	The procedure is outlined is algorithm~\ref{algo:outer_basic_al_trm} 
	
	\begin{algorithm}
		\caption{Outer iteration of basic AL algorithm with trust region}\label{algo:outer_basic_al_trm}
		\begin{enumerate}
			\item[\textbf{Step 0: Initialization}]
			
			Choose a feasible starting point $x_0$, vector of Lagrange multipliers estimates $\lambda_0$. 
			
			Set the penalty parameter to $\mu_0>1$, the tolerances $\omega_0>0, \eta_0 > 0$. Start the  iteration index $k$ at $0$.
			\item[\textbf{Step 1: Inner Iteration}]
			
			Starting from $x_k$, approximately solve \[\min_{x\in \calX} \Phi_A(x,y_k,\mu_k)\] by computing $x_{k+1}$ verifying condition~\eqref{eq:al_approximate_optimality}.
			
			\textbf{If} $\|h(x_k)\| \le \eta_k$, execute \textbf{Step 2}.
			
			Otherwise, execute \textbf{Step 3}.
			
			\item[\textbf{Step 2: Iterate Update}]
			
			Update $\lambda_{k+1} \gets \lambda_k + \mu_kh(x_k)$ and set $x_{k+1}^s \gets x_k $.
			
			Choose new tolerances \(\omega_{k+1}, \eta_{k+1}\) and new penalty parameter \(\mu_{k+1} > \mu_k\).
			
			Increment $k$ and go back to \textbf{Step 1}.
			
			\item[\textbf{Step 3: Adjustment of the Penalty Parameter}] 
			
			Choose $\mu_{k+1}$ significantly greater than $\mu_k$ and new tolerances \(\omega_{k+1}, \eta_{k+1}\).
			
			Leave the multipliers and tolerances unchanged: $\lambda_{k+1} \gets \lambda_k$.
			
			Increment $k$ Go back to \textbf{Step 1}.
		\end{enumerate}
	\end{algorithm}
	
	\subsection{The inner iteration}
	
	For a point $x$, we define $\calA(x):=\left\{i\ \vert \ x_i \in \{\bar{l}_i,\bar{u}_i\}\right\}$ the indices of bounds constraints active at $x$. This set mixes the bounds from the problem and the trust region.
	
	To compute the step, we follow the approach of~\citet{linmore:1999a} and successively apply the conjugate gradient method and adaptively modify the active set if we encounter a bound or the trust region.
	
	In order to do so, we compute $m+1$ minor iterates $x_{k,1}, \ldots, x_{k,m+1}$ with $x_{k,1}=x_k$ and $p_k=x_{k,m+1}-x_k$. Each minor iterate is updated by $x_{k,j+1}=x{k,j}+p_{k,j}$, where $p_{k,j}$is an approximate solution the QP
	\begin{equation}\label{eq:minor_iterate_subpb}
		\begin{aligned}
			\min_{p} \quad& \calQ_k(p)  \\
			\text{s.t.}  \quad &  Cp=0\\
			& p_i=0,\quad i\in \calA(x_{k,j}),
		\end{aligned}	
	\end{equation}
	starting from $x_{k,j}$. Subproblem~\eqref{eq:minor_iterate_subpb} is solved by the projected conjugate gradient method with early stopping if we encounter a bound or the trust region. The next active set $\calA(x_{k,j+1})$ is formed after $\calA(x_{k,j})$ and the bounds newly active.  

	\subsection{Questions}
	\begin{itemize}
		\item Would it be interesting to start the inner from the Cauchy point? If yes how to compute it, i.e. how to project the steepest direction on the feasible set?
		\item Which approximation of the Hessian $\nabla^2_{xx} \Phi_A(x_k,\lambda_k,\mu_k)$?
		\item Rules for the update of the penalty parameter, the tolerances, the trust region radius?
	\end{itemize}
	\clearpage
	
	\bibliographystyle{plainnat}
	\bibliography{refs}
	
\end{document}